{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7733be30",
   "metadata": {},
   "source": [
    "# Modern RNN Units - GRU and LSTM\n",
    "- LSTM = Long Short-Term Memory\n",
    "- GRU = Gated Recurrent Uniy\n",
    "- they are basically same thing, GRU is more simplified and efficient version of LSTM\n",
    "\n",
    "### Why simple RNN was not enough in the first place?\n",
    "- Gradient vanishing problem\n",
    "- see in the equation Wxh appears many times, in every timestamp\n",
    "- we'll need the gradient of Wxh.T * xt for all t= 1, .. T\n",
    "- the final gradient will be a function of all these individual gradients\n",
    "- also consider how deeply each of these terms are nested\n",
    "- x1 is the most deeply nested, x2 is the next\n",
    "- now remember the derivative of composite functions - chain rule\n",
    "- the more deeply nested, the more multiplications\n",
    "- RNNs are particularly vulneranle to the vanising gradient problemi the deeper nested, the more vulnerable\n",
    "- hence the RNNs cannot learn from inputs that are too back\n",
    "#### you might think why we don't solve it again by using ReLu?\n",
    "- with RNNs, it's not that simple\n",
    "- in DL we found out the best way to deal with this problem is GRU and LSTM\n",
    "- interstingly LSTM was found in 1997 way befoe than DL called DL\n",
    "- GRU invented in 2014, uses same principles but simpler\n",
    "\n",
    "## Gated Recurrent Unit (GRU)\n",
    "- it has the same API as simpleRNN: hidden state t will still depend on the current input Xt and previous hidden state ht-1\n",
    "- the only thing that changes is how ht is calculated\n",
    "- GRU equations ın ipad, it contains three equations bcz we have to calculate three different things:\n",
    "    - z(t) = update gate vector\n",
    "    - r(t) = reset gate vector\n",
    "    - h(t) = hidden state vector (what gets passed on the next layer)\n",
    "- First let's consider shapes, all three: z(t), r(t), h(t) are vectors of size M.\n",
    "- M as usual is hyperparameter\n",
    "- This also implies shapes of weights\n",
    "- any weight going from xt has shape DxM\n",
    "- any weight going from ht-1 has size MxM\n",
    "- all bias terms are size M\n",
    "\n",
    "### z(t) : Update gate\n",
    "- telling us what should I do? should I take the new value for h(t) or should I keep the old value h(t-1)?\n",
    "- so whether or not to remember past hidden state\n",
    "- we can explicitly remember the past state h(t-1) if zt is small.\n",
    "- since z(t) is output of a sigmoid, it's value is always btw zero and one\n",
    "#### so thx to the presence of z(t), when calculating each h(t) we're taking the weighted sum of the previous h(t-1) and this other function (which is analogous to what simpleRNN is doing)\n",
    "- look again at the equation of z(t), it's just a neuron! (logistic reg fnction)\n",
    "- note that the sigmoid here is not just treated as a hyperparameter, it's always sigmoid cause we want z(t) btw 0&1\n",
    "- so what this neuron predicts?\n",
    "- it's actually like a binary classifier which tells us which thing to choose to get h(t)\n",
    "- but we never acytually keep or discard, z(t) gives us a probability so we actually take a mixture of 2 components\n",
    "- this also called, convex combination, soft mix or soft seleciton\n",
    "### r(t): Reset gate\n",
    "- it's just another neuron, just another switch to remmebr forget h(t-1)\n",
    "- r(t) appears therefore actually the second component of h(t) is not just a simpleRNN\n",
    "- as you see, bcz of sigmoid it's also always btw zero and 1\n",
    "- so when it multiplies h(t-1) as in the equation the value will get closer to zero when r(t) get closer to zero.\n",
    "- so before multiplyhing the h(t-1) with the weight matrix Whh.T, we first decide which part of the h(t-1) we wanna remember and which part we wanna forget, by resetting them to zero.\n",
    "- it's just reinforcing our ability to remember and forget different parts of h(t-1)\n",
    "- BURDA different parts'dan kasıt ne ya?\n",
    "\n",
    "### Summary\n",
    "- GRU has gates to remember / forget each component of h(t-1)\n",
    "- SimpleRNN no choice but eventually forget bcz of vanishing gradeint\n",
    "- we use binary classifiers (logistic regression neurons) as our gates\n",
    "\n",
    "## Long Short-Term Memory (LSTM)\n",
    "- when GRU first came out, there was no clear winner btw the two\n",
    "- they said performed comparably so GRU better choice bcz less parameters > more performant\n",
    "- currently not believed this, shifted in favor of LSTM\n",
    "- so it's always experimentation no phiolosphical deduction, he added to articles in extra reading txt on this: \n",
    "    - On the Practicial Computational Power of the Finite Precision RNNs for Language Recognition\n",
    "    - Massive Exploration of Neural Machine Translation Architectures\n",
    "- they both say LSTM outperforms GRUs, so he'd probably choose LSTM by default\n",
    "- LSTM work like GRU but with more vectors and gates, also not exactly the same API\n",
    "- LSTM returns two states:\n",
    "    - hidden state h(t)\n",
    "    - Cell state c(t) - usually ignored\n",
    "- sometimes cell state is considered an idditional hidden state so you actually pass this on to the next layer, but we usually ignore\n",
    "- LSTM unit in Tensorflow outputs h(t) but can optionally also output c(t)\n",
    "- also for inputs you'll need two initial states c0 and h0\n",
    "- LSTM equations : ipad\n",
    "    - f(t) = forget gate vector\n",
    "    - i(t) = input/upddate gate vector analogously to GRU\n",
    "    - o(t) = output gate vector\n",
    "    - c(t) = cell state\n",
    "    - h(t) = hidden state\n",
    "- Ct (like ht in GRU) is a weighted sum of two terms: the previous cell state c(t-1) , how much of this we'll keep will be determined by f(t) forget gate, it controls how much of c(t-1) we wanna fforget. the second term is the simpleRNN, how much of this we're gonna keep is controlled by input gate i(t). fc in this second part of equation represent an activation function, it can be anything but generally tanh. \n",
    "- ABİ KAFA KARIŞSIN DİYE Mİ ONDA DA f HARFİ KULLANDINIZ\n",
    "- similarly fh is just another activation function which can again be anything but by default usually tanh\n",
    "- output gate controls which of the values of the cell state pass through to the hidden state h(t).\n",
    "#### Note: fc and fh in Tensorflow and Keras are tanh by default, and cannot be changed indivudually as both are controlled by the activation argument, so if you change activation to ReLu, they'll both be RelU. you cannot change just one unless you build your own LSTM\n",
    "#### what we want is to offer the cell state the possiblity to remember its old state so that we can learn the longterm dependencies\n",
    "- Note, f(t), i(t), o(t) are neurons as binary classifiers.\n",
    "- c(t) = f(t) * c(t-1) + i(t) * SimpleRNN\n",
    "- h(t) = o(t) * tanh(c(t)) #### most of the time tanh, it's a hyperparameter you don't need to play around much indeed\n",
    "#### if you change tanh, your LSTM unit won't use GPU, incompatible with GPU , at least in tis version of Tensorflow\n",
    "- there are some other requirements too for LSTM to be GPU compatible you may wanna check out\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ba7b8b1",
   "metadata": {},
   "source": [
    "# In code:\n",
    "\n",
    "i = Input(shape = (T,D))\n",
    "x = LSTM(M)(i)\n",
    "x = Dense(K)(x)\n",
    "model = Model(i,x)\n",
    "\n",
    "#as simpleRNN, GRU and LSTM also returns only hT by default\n",
    "# we may want all h1,h2,h3.. to get y1,y2,..yT\n",
    "i = Input(shape = (T,D))\n",
    "x = LSTM(M, return_sequences = True)(i) #NxTxM\n",
    "x = Dense(K)(x) #NxTxK\n",
    "\n",
    "\n",
    "###another intersting thing is the return_state argument\n",
    "o,h = SimpleRNN(M, return_state = True)(i) #o and h are the same thing - final state ht\n",
    "o,h = GRU(M, return_state = True)(i) #o and h are the same thing - final state ht\n",
    "o,h = LSTM(M, return_state = True)(i) #o and h are the same thing - final state ht, but we also get cT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c105c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
