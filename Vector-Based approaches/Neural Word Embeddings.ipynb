{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a97b71",
   "metadata": {},
   "source": [
    "# Neural Word Embeddings\n",
    "\n",
    "- word embedding is just a fancy word for saying vector in our purposes at least\n",
    "- neural word embeddings convert single words into vectors instead of full docs\n",
    "- hence a much more fine grained representation ? \n",
    "- thus a document becomes a whole sequence of vectors\n",
    "- this contains much more info than bag-of-words\n",
    "\n",
    "### sequences of vectors\n",
    "in DL we have models for sequences\n",
    "\n",
    "### 2 methods to discuss for word embeddings:\n",
    "- word2vec (google)\n",
    "- GloVe(Stanford)\n",
    "\n",
    "## Word2Vec\n",
    "- embeddings are stored in the weights of the neural network\n",
    "- the goal: predict, given an input word, whether or not another word would be found in its context\n",
    "- context = a small window surrounding the input word\n",
    "- örnek var deste, 24.derse bak. a binary pred yapıyo\n",
    "\n",
    "## Glove\n",
    "- doesn't use neural network but became popular in the era of DL in NLP\n",
    "- and embeddings are still used in NN at later stages\n",
    "- hence still considered part of DL\n",
    "- work like a recommender system\n",
    "- ex: Look for patterns in which users liked movies\n",
    "- take the existed ratings and try to predict what viwers rate on the movies not yet seen\n",
    "- again the idea of context\n",
    "\n",
    "word2vec'teki mantıkla kontexte bakıcaz, we pretend that users are words and movies are also words\n",
    "\n",
    "our ratings are based on distances between the words\n",
    "\n",
    "ex: The quick brown fox jumps over the lazy dog.\n",
    "\n",
    "fox is one word away from jump score:1/1\n",
    "\n",
    "brown is 2 words away from jump score:1/2\n",
    "\n",
    "ratings are the distances between the words, bcz of this analogy we can evaluate distances and recommend movies\n",
    "\n",
    "### What can we do with word embeddings? \n",
    "\n",
    "#### One thing to do : word vectors\n",
    "- can convert a document into vectors but unlike in tf-idf or count vectors, these won't be sparse or high dimensional\n",
    "- embeddings are dense and low dimensional, this vectors are relatively smaller thn other vectors from this section\n",
    "\n",
    "First we tokenize all words \n",
    "\n",
    "Then we map all of them into word vectors\n",
    "\n",
    "Then average them to get a single vector of the same size\n",
    "\n",
    "This give us ome vector that represents the whole document without regarding the order of the words, THUS ANOTHER BAG-OF-WORDS rep\n",
    "\n",
    "#### Another: Word analogies\n",
    "perhaps the most şmpresive result of word embedding tehniques\n",
    "- because we have a set of vectors, we can do arithmetic in these vecotrs + / -\n",
    "- consider the analogy what is king is to men, what is to women? answer: queen\n",
    "- It turns out we can express this anology in math!!!\n",
    "- In math : vector for King- vector for man = vector for Queen - vector for Woman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In code:\n",
    "x = King - Man + Woman\n",
    "#Find the closest vector word to x, the answer will be queen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc20e1",
   "metadata": {},
   "source": [
    "we can do this sort of analogies for many things\n",
    "\n",
    "France : Paris :: Italy : Rome\n",
    "\n",
    "Japan : Japanese :: China : Chinese\n",
    "\n",
    "December : November :: July : June\n",
    "\n",
    "Man : Woman :: He : She\n",
    "\n",
    "#### we're able to learn some sense of meaning for each of these words by their position in vector space !\n",
    "so if we wee to use these vectors in a NN, we'll know that those NN has close-to-the-optimal weights, perhaps won't need to train the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28066161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
