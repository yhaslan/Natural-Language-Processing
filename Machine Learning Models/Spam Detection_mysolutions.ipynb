{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a675d47",
   "metadata": {},
   "source": [
    "# Spam Detection\n",
    "\n",
    "- we'll use the Naive Bayes Intuiton Technique for Spam Detection\n",
    "\n",
    "## Naive Bayes Intuition\n",
    "- assume that inputs are independent from each other\n",
    "- this is what makes it naive?\n",
    "- what kind of distribution to pick? \n",
    "- totally up to you\n",
    "- you can also pick different distributions for different inputs\n",
    "- ex: X1 Gaussian, x2 Exponential, x3 Bernoulli\n",
    "- In Sklearn we have predefined models, all inputs come from the same kind of distribution (Gaussian, Multinomial, Bernoulli): you cannot choose different distributions for your Xs\n",
    "- how do you choose? #by looking t your data\n",
    "- if your data is continuos and looks like a bell curve: Gaussian, if categorical: multinomial. the latter is typically the correct option for NLP, specifically for count vectors and Tf-idf\n",
    "- why tfidf de good option? anlamadım\n",
    "- if data is binary bernoulli would be good choice, this may be applicable if you choose a binary version of count vector sfor nlp\n",
    "\n",
    "## Exercise Prompt\n",
    "- a dataset of sms, labeled spam and non spam\n",
    "- build a classifier, split into train and test dataset, assess accuracy\n",
    "\n",
    "### Details\n",
    "- vectoirizng strategy of your choice\n",
    "- options:  tokenization, lemmatization, normalization\n",
    "- classifier: an appropriate form of Naive Bayes, either from sklearn or if you're advanced build yourself\n",
    "- feel free to try other classifiers\n",
    "- score function returns accuracy\n",
    "- not ideal when classes are imbalanced, \n",
    "- if imbalanced check other metrics function like AUC / F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62d8ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-14 00:07:03--  https://lazyprogrammer.me/course_files/spam.csv\n",
      "Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166\n",
      "Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 503663 (492K) [text/csv]\n",
      "Saving to: 'spam.csv.2'\n",
      "\n",
      "spam.csv.2          100%[===================>] 491.86K   505KB/s    in 1.0s    \n",
      "\n",
      "2023-06-14 00:07:08 (505 KB/s) - 'spam.csv.2' saved [503663/503663]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data from:\n",
    "# https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "!wget https://lazyprogrammer.me/course_files/spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfabb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk #kirill'de bunla yaptık\n",
    "from nltk.stem.porter import PorterStemmer #kirill\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB #kirill'le olan positive negative sentiment ama Gaussian kullanmış\n",
    "#bu dersin hocası Multinomial kullanıyo :/\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score #kirill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed1acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yagmuraslan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680ae2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\")\n",
    "df.head()\n",
    "\n",
    "#excelde comma-delimited ufc-8 olarak kaydettikten sonra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f7e5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                Will Ã_ b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: v2, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"v1\",\"v2\"]]\n",
    "df[\"v2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36312f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df[\"v2\"]\n",
    "labels = df[\"v1\"]\n",
    "type(labels)\n",
    "labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef9d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (labels == \"spam\")\n",
    "labels = labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbdfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, labels_train, labels_test = train_test_split(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984992f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725cff75",
   "metadata": {},
   "source": [
    "### Önce Word_tokenize kullanak istedim çünkü bence punctuation ve special karakterler sonucu etkileyebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e0aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d803bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = vectorizer.fit_transform(text_train)\n",
    "Xtest = vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c42a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(Xtrain, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394283d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9932998324958124\n",
      "test score: 0.9791816223977028\n"
     ]
    }
   ],
   "source": [
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d345786",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = classifier.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991182ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1203    7]\n",
      " [  22  161]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9791816223977028"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(labels_test, labels_pred)\n",
    "print(cm)\n",
    "accuracy_score(labels_test, labels_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6004c1",
   "metadata": {},
   "source": [
    "- train score: 0.9932998324958124\n",
    "- test score: 0.9791816223977028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda86aaa",
   "metadata": {},
   "source": [
    "### Şimdi count vectorizer'ın default tokenizerı ile deneyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "337efc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9944962909787031\n",
      "test score: 0.9842067480258435\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "Xtrain = vectorizer.fit_transform(text_train)\n",
    "Xtest = vectorizer.transform(text_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(Xtrain, labels_train)\n",
    "\n",
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ee8ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1203    7]\n",
      " [  15  168]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9842067480258435"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = classifier.predict(Xtest)\n",
    "cm = confusion_matrix(labels_test, labels_pred)\n",
    "print(cm)\n",
    "accuracy_score(labels_test, labels_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2819f74",
   "metadata": {},
   "source": [
    "- train score: 0.9944962909787031\n",
    "- test score: 0.9842067480258435"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c38e36",
   "metadata": {},
   "source": [
    "### Şimdi Tf-idf vectorizer ile deneyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a93de35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9827709978463748\n",
      "test score: 0.9741564967695621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 2000)\n",
    "Xtrain = vectorizer.fit_transform(text_train)\n",
    "Xtest = vectorizer.transform(text_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(Xtrain, labels_train)\n",
    "\n",
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32fb1524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1209    1]\n",
      " [  35  148]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9741564967695621"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = classifier.predict(Xtest)\n",
    "cm = confusion_matrix(labels_test, labels_pred)\n",
    "print(cm)\n",
    "accuracy_score(labels_test, labels_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a69de",
   "metadata": {},
   "source": [
    "### şimdi countvectorizer'ı stopwords ile deneyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "098d9717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9944962909787031\n",
      "test score: 0.9820531227566404\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "Xtrain = vectorizer.fit_transform(text_train)\n",
    "Xtest = vectorizer.transform(text_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(Xtrain, labels_train)\n",
    "\n",
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "977b1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## default count vectorizer ile hiçbir fark yaratamadı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d13814",
   "metadata": {},
   "source": [
    "### şimdi countvectorizer'ı  lemmatization ile deneyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75e6d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa9ea138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yagmuraslan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yagmuraslan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yagmuraslan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yagmuraslan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32274363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "  if treebank_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8f7a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "  def __init__(self):\n",
    "    self.wnl = WordNetLemmatizer()\n",
    "  def __call__(self, doc):\n",
    "    tokens = word_tokenize(doc) #this converts doc to tokens, like a fancier version of string split\n",
    "    words_and_tags = nltk.pos_tag(tokens) #this obtains pos tags, returns a tuple\n",
    "    return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) \\\n",
    "            for word, tag in words_and_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afe1f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.992581957406078\n",
      "test score: 0.9784637473079684\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer= LemmaTokenizer())\n",
    "Xtrain = vectorizer.fit_transform(text_train)\n",
    "Xtest = vectorizer.transform(text_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(Xtrain, labels_train)\n",
    "\n",
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216495b",
   "metadata": {},
   "source": [
    "### şimdi de stemmer ile deneyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1ddfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def __call__(self,doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        return(self.porter.stem(t) for t in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e82ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9932998324958124\n",
      "test score: 0.9784637473079684\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer= StemTokenizer())\n",
    "Xtrain = vectorizer.fit_transform(text_train)\n",
    "Xtest = vectorizer.transform(text_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(Xtrain, labels_train)\n",
    "\n",
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c9d0f",
   "metadata": {},
   "source": [
    "#### en iyi sonucu sanki count vectorizer'ın default'u ile aldım.\n",
    "## Bi de Gaussian olsa nolur acaba bakalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd457c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/9bnp6m5d7db0pky3235l4wgh0000gn/T/ipykernel_13524/3790022056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         return self._partial_fit(\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0;34m\"A sparse matrix was passed, but dense \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;34m\"data is required. Use X.toarray() to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "Xtrain = vectorizer.fit_transform(text_train)\n",
    "Xtest = vectorizer.transform(text_test)\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(Xtrain, labels_train)\n",
    "\n",
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07c96ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9373055754965303\n",
      "test score: 0.8894472361809045\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "Xtrain = vectorizer.fit_transform(text_train).toarray()\n",
    "Xtest = vectorizer.transform(text_test).toarray()\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(Xtrain, labels_train)\n",
    "\n",
    "print(\"train score:\", classifier.score(Xtrain, labels_train))\n",
    "print(\"test score:\", classifier.score(Xtest, labels_test))\n",
    "\n",
    "### BAYA KÖTÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032710eb",
   "metadata": {},
   "source": [
    "### AY ÇOK GARİP Bİ ŞEY FARK ETTİMMMMM!!!! \n",
    "## KİRİL TRAİN TEST SPLIT'TEN ÖNCE YAPIYOR VECTORIZATION'I BİZSE SONRA YAPIYORUZ, ARTILARI EKSİLERİ NELERDİR?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b03db6f4",
   "metadata": {},
   "source": [
    "# F1 SCORE\n",
    "Ptrain = model.predict(Xtrain)\n",
    "Ptest = model.predict(Xtest)\n",
    "print(\"train F1:\", f1_score(Ytrain, Ptrain))\n",
    "print(\"test F1:\", f1_score(Ytest, Ptest))\n",
    "\n",
    "## ROC AND AUC\n",
    "Prob_train = model.predict_proba(Xtrain)[:,1]\n",
    "Prob_test = model.predict_proba(Xtest)[:,1]\n",
    "print(\"train AUC:\", roc_auc_score(Ytrain, Prob_train))\n",
    "print(\"test AUC:\", roc_auc_score(Ytest, Prob_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b8184",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes (based on the Gaussian distribution) is suitable when dealing with continuous numerical features that follow a Gaussian (normal) distribution. If your input features are continuous variables (e.g., word frequencies) and exhibit a roughly Gaussian distribution, Gaussian Naive Bayes can be a reasonable choice.\n",
    "\n",
    "- On the other hand, if your input features are discrete or represent counts (e.g., word occurrences), Multinomial Naive Bayes is more appropriate. Multinomial Naive Bayes assumes that features follow a multinomial distribution, which is often the case with text data or categorical variables.\n",
    "\n",
    "- In the context of spam detection, text data is commonly used, and Multinomial Naive Bayes is a popular choice. It allows you to represent emails as vectors of word frequencies or presence indicators, making it well-suited for text classification tasks.\n",
    "\n",
    "- Therefore, for spam detection using a Naive Bayes classifier, the preferred choice would typically be the Multinomial Naive Bayes classifier. However, it is recommended to experiment and evaluate both Gaussian and Multinomial Naive Bayes classifiers on your specific dataset to determine which one performs better in terms of accuracy and meets your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7312ed",
   "metadata": {},
   "source": [
    "#### Bernoulli Distribution\n",
    "- Certainly! One example where you may want to use the Bernoulli distribution with a Naive Bayes classifier is in sentiment analysis or text classification tasks where the focus is on the presence or absence of specific features (words or binary indicators) in the text.\n",
    "\n",
    "- For instance, consider a sentiment analysis task where you want to classify movie reviews as positive or negative based on the presence of certain words in the review. In this case, you can represent the input data as a binary matrix, where each row corresponds to a movie review and each column represents the presence or absence of a specific word.\n",
    "\n",
    "- The Bernoulli Naive Bayes classifier is well-suited for this scenario because it assumes that the features are binary variables (e.g., word presence indicators) and models the conditional probabilities of the features given the class label. It calculates the likelihood of a feature being present or absent in each class and combines this information with the prior probabilities of the classes to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ae97a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
