{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7604df60",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "- A CNN is nothing but a NN with convolution\n",
    "- our discussion on convolution will start with images\n",
    "- we will move on to texts later on\n",
    "\n",
    "- An image is 2-d continuous signal whereas a time-series is a 1-d continuous signal\n",
    "- then we can convert text into vectors, that are also time-series but with multiple features\n",
    "- thus using CNN for NLP is the same as using CNN for time-series\n",
    "#### Images are considered 2D even though they're stored in 3-D arrays.\n",
    "\n",
    "### What is convolution?\n",
    "- it's basically about adding and multiplying\n",
    "- Input image * Filter (=kernel) = output image\n",
    "- output image is what you get when you co-involve the input image with the filter. \n",
    "- In other words if I perform a convolution operation on the input image and filter I will get the output image\n",
    "- Convolution = Image Modifier, you may think it as a feature transformation on the input image\n",
    "- When you use a Gaussian filter: you get a blurred image\n",
    "- when you use and edge detection filter : you get a sharpened image\n",
    "#### filter application:\n",
    "- if the input length = N, kernel length = K, output length = N - K + 1, you can put the filter into.\n",
    "- output_length = input_lentgh - kernel_length + 1\n",
    "- ouput_heighth = input_heigth - kernel_heigth + 1\n",
    "- but are images always square? not really, but sometimes when you build dataset, you make them squre to train easier\n",
    "- filter / kernel are almost always square\n",
    "\n",
    "### pseudocode for convolution:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f43bc54b",
   "metadata": {},
   "source": [
    "output_image = np.zeros((output_height, output_width))\n",
    "for i in range(0, output_height):\n",
    "    for j in range(0,output_width):\n",
    "        for ii in range(0, kernel_height):\n",
    "            for jj in range(0, kernel_width):\n",
    "                output_imaje[i,j] += input_image[i+ii,j+jj] * kernel[ii,jj]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61836f3",
   "metadata": {},
   "source": [
    "Scipy library has already a function for convolution"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a865d1ab",
   "metadata": {},
   "source": [
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d8fba",
   "metadata": {},
   "source": [
    "- but if you apply this function you'll get a completely diffrent answer from what we did\n",
    "- bcz scipy apply true convolution and not deep learning version convolution with + instead -. \n",
    "- in order to make it dl convolution, flip the filter vertically and horizontally and use mode = valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0706a05",
   "metadata": {},
   "source": [
    "convolve2d(A, np.fliplr(np.flipud(w)), mode = 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e631a",
   "metadata": {},
   "source": [
    "what we're doing in DL is actually called cross-correlation\n",
    "- what is mode anyway?\n",
    "- the movement of the filter is bounded by the edges of image\n",
    "- hence output image always smaller than input image\n",
    "\n",
    "### Padding the \"same\" mode\n",
    "- what if we want our output to be same size as input\n",
    "- then we can use padding: add imaginary zeros around the input\n",
    "#### even more padding! (\"full\" padding mode)\n",
    "- we can extend the filter and still get non-zero outputs\n",
    "- you can extend the padding so you can catch all these nonzero outputs\n",
    "- this results in an output size N+K-1\n",
    "### summary of modes:\n",
    "- valid : N-K+1, typically used\n",
    "- same : N , typically used\n",
    "- full: N+K-1 , not typical\n",
    "\n",
    "### Alternative Perspective on Convolution: Pattern Matching\n",
    "- why is dot product important?\n",
    "- we sometimes refer this cosine similarity or cosine distance depending on the sign\n",
    "- look at the ipad\n",
    "- cosine similarity : min value = -1, max value = 1\n",
    "- cosΩ = a dot b / |a| * |b|\n",
    "- gives us how similar a and b\n",
    "- so hope you're convinced that the dot product can be used as a correlation measure\n",
    "- if they're highly positively correlated, the dot product should be large and positive\n",
    "- if they2re highly negatively correlated, it should be large and negative\n",
    "- if orthogonal (no correlation) - dot product should be zero\n",
    "\n",
    "#### so in fact what kernel/filter does is to filter out everything that is not related to the pattern contained in the filter by sending them to zero. It slides throu the image looking for the pattern, and it gives us high number for where the patterns found and small number where the pattern is not found\n",
    "\n",
    "\n",
    "### Alternative Perspective on Convolution: Weight Sharing\n",
    "- how to see convolution as matrix multiplication\n",
    "- filter'ı kenarlara sıfır koyup kaydırmak suretiyle tekrar tekrar yazarak bi matrix yapma\n",
    "- ama burda vector of size 2 olan filtre bi anda 3x4 matrikse dönüşüyo gerek var mıydı?\n",
    "- instead consider the problem from the opposite direction\n",
    "- can we turn matrix multiplications into convolution? especially when it's big and slow?\n",
    "#### this is the idea behind parameter sharing or weight sharing\n",
    "- we do this operation inside a matrix multiplication: a = W.T * x\n",
    "- what if instead of weight matrix we had used the same weights repeating over and over again?\n",
    "- then we could have had less parameters, use less RAM, and make computation more efficient\n",
    "- convolution saves space and time by using less weights\n",
    "#### why do this?\n",
    "- consider a fully connected ANN,\n",
    "- MNIST: 28x28 = 784-sized input vector\n",
    "- CIFAR-10 : 32x32x3 = 3072-sized input vector\n",
    "- VGG(a modern CNN) looks at images of size 224x224!\n",
    "- suppose an image with modern HD resolution: 1280x720\n",
    "- you have about 2.8 million features - tooo large for NN\n",
    "#### also another good reason to use weight sharing:\n",
    "- remembr convolution is pattern finder\n",
    "- we want the same filter to look all locations in an image\n",
    "- this is the idea behind \"translational invariance\"\n",
    "#### Translational Invariance\n",
    "- suppose building a dog vs cat recognizer\n",
    "- consider two equal cats only positioned differently in images\n",
    "- if you use fully connected or dense NN, we would have to learn weights for each of these positions separately\n",
    "- and moreover, this neuron won't generalize well bcz if we came across the same cat in a new position, it will fail to recognize it.\n",
    "- so it's better to have a shared pattern finder bcz this pattern finder looks at all locations in the image\n",
    "- no need to learn the weights to look at a cat on every possible point\n",
    "\n",
    "## Convolution on Color Images\n",
    "- images are normaly 3-d objects: height x weight x color\n",
    "- if our image has 3-dimensions, so should our filter, cube instead of square\n",
    "- 2D dot product is a grayscale pattern finder\n",
    "- 3D dot product is a color pattern finder\n",
    "- three colors: red, green, blue\n",
    "- Input image = H x W x 3\n",
    "- Filter = K x K x 3\n",
    "- Output = (H - K + 1) x (W- K + 1) ##the output image is only two dimensional and it's because we're summing over two dimensions and third dimension kinda get sorted out\n",
    "- but remember we look for uniformity in NNs. repeated structure\n",
    "- in such case, how can we add another convolution layer?\n",
    "\n",
    "### or consider this: you wanna look for multiple features\n",
    "- ex : one for eye, one for nose\n",
    "- then you need multiple filters\n",
    "- imagine you have two filters acting on the same input image\n",
    "- A.shape = HxWx3\n",
    "- if we use \"same\" mode, then B1.shape = HxW, B2.shape = HxW - we have two outputs with different features found in eacg\n",
    "- if we stack togethr B1 and B2, we get HxWx2! all of a sudden the output is three dimensional\n",
    "Stack ??? \n",
    "- we can add any number filters to find any number of features!\n",
    "- now our inputs are like our outputs 3-D\n",
    "- Let's vectorize this operation, we don't need to do each convolution separately.\n",
    "- farklı filtreleri aynı anda w diye mi geçircez?\n",
    "\n",
    "#### summary so far:\n",
    "- we first talked about only 2-d convolutions (gray scale)\n",
    "- we extended to color images by saying the filter should also have the same depth, then we dot along all 3 axes\n",
    "- this broke uniformity, inputs are 3-d while outputs were 2-d\n",
    "- how can stack multiple convolutions in sequence then?\n",
    "- well, realized each layer should actually find multiple features(using multiple filters)\n",
    "- then the output will be 3-d, 2-d outpurs oer filter.\n",
    "\n",
    "#### then is it really correct to call final dimension color?\n",
    "- when the input to NN is a true color image yes\n",
    "- but after subsequent convolutions we just have HxWx(arbitrary)\n",
    "- change the terminology to cover more general case (third dimension no longer represents just color)\n",
    "- we'll call those \"feature maps\", e.g. each 2-d image is a map that tells us where the feature is found , they're the result of some convolution looking for some feature, 1 map for each feature\n",
    "- so the size of final dimension we call nuber of channels or number of feature maps rather than colors\n",
    "\n",
    "### Convolution Layer\n",
    "- how convolutions gonna look as a NN layer?\n",
    "- remember : they're like a shared-weight version of matrix mjultiplication (e.g. in a Dense LAyer)\n",
    "- but remember: weights are not the only thing in a dense layer, there is also bias term and an activation function\n",
    "- well convolution layer should have those too!\n",
    "- we're gonna add a base term and apply an activation function on convolution\n",
    "- convolution layer = ö(W*x + b) , dense layer = ö(W.T*x + b) conv'da transpoze yok mu?\n",
    "- importantly we have to cosider shape of bias term, given that the output of convolution is a 3-d image\n",
    "- in a Dense Layer, if W.T is a vector of size M, then b is also vector of size M\n",
    "- yet , in convolution b does not have the same shape as W*x (a 3-d image)!!!!!!\n",
    "- well technically it shouldn't be alloed by the rules of arithmetic\n",
    "- but when you actually working with the code, this actually allowed due to the broadcasting (in Numpy code)\n",
    "- so if W*x has shape HxWxC2 the bias term has only one dimesion and its size is C2.\n",
    "#### In other words, the same bias term is applied to each pixel for every picture\n",
    "- one scalar for feature map!\n",
    "- In convolution, these all are just conceptual so doesn't matter if W*x or x*W but it's not entirely true in DL case\n",
    "- but just pretend W comes early so it will look more like dense layer equation\n",
    "\n",
    "#### Let's calculate how much we save by doing convolution instead of matrix multiplication\n",
    "- suppose input : 32x32x3\n",
    "- filter: 3x5x5x64 (so there are 64 output feature maps)\n",
    "- output image: 28x28x64 (32-5+1) let's assume we're using valid mode\n",
    "- number of parameters needed (ignoring the bias term): 3x5x5x64 = 4800\n",
    "\n",
    "- now consider a dense layer (full matrix multiply):\n",
    "- flattened input image: 32x32x3 = 3072\n",
    "- flattened output image: 28x28x64 = 50176\n",
    "- if we use the dense layer, the weight matrix size = 3072 x 50176 = 154,140,672!!!!\n",
    "- 32,000 more paarameters compared to convolution\n",
    "- it will take longer, use more ram and could potentially also perform suboptimally because we wanna use the SAME pattern finder in multiple places!\n",
    "- without shared weights, we'd need to learn to find the pattern in every location it may appear separately\n",
    "- this would be very bad for generalization\n",
    "\n",
    "#### how are convolution filters found?\n",
    "- W will be found the same way as before, automatically ! when we call the model.fit\n",
    "- this is gonna end up doing gradient descent on our new weights and biases, which for convolution happen to have a shape different than before what we had in dense layer.\n",
    "\n",
    "\n",
    "\n",
    "## CNN Architecture\n",
    "- moden CNNs are all originated from the same model LeNet\n",
    "- I'll draw a typical CNN on ipad\n",
    "- typical CNN is made of two stages\n",
    "- the first stage:  a series of convolution layers, usually followed by pooling layers\n",
    "- second stage: a series of dense layers, also called fully conneccted layers\n",
    "- CNN has hierarchical structure, each layer is looking at the features found in the prev layer\n",
    "- you can think of first stage as a \"feature transformer\" (that specifically works on images and find image features)\n",
    "- once it finds those features, it passes to dense layers, which does its best non-linear classification or regression\n",
    "- second stage can be considered as nonlinear classifier of features\n",
    "### Pooling:\n",
    "- at a high level pooling acts like a downsampling operation: output a smaller image from a bigger image\n",
    "- If input is 100x100 a pool of size 2 would result in 50x50\n",
    "- aka downsampling by 2\n",
    "- but why it works and why we wanna include it in our NN?\n",
    "#### there are 2 kinds of pooling: Max pooling and Average Pooling\n",
    "- choosing whcich one to use is obviously a hyperparameter choice\n",
    "- max pooling : ipad'e çizcem\n",
    "- max is a bit more common\n",
    "- average pooling siilar, instead of max, it takes the average\n",
    "- It's practical cause if we shrink the image we have less data to process\n",
    "- Translational Invariance: Idc where in the image the feature occured, i just care it did\n",
    "- Recall, convolution is pattern finder. tells us where the pattern is found and not found, so the highest number is the best matching location. without caring where the pattern is found, pooling tells us it was found.\n",
    "- we can select different pool sizes: it is not mandatory to be same size in both directions- rectangular olabilir\n",
    "- but non-square windows are usualyy unconventional\n",
    "- IT'S POSSIBLE FOR THE WINDOWS TO OVERLAP! The hyperparameter controlling this is called \"stride\". stride tells us how many spaces we're moving the pooling box for each output.\n",
    "### why should we have a sequential pattern where each conv layer is followed by a pooling layer?\n",
    "- one cool thing researchers discovered is CNN able to learn features hierarchically\n",
    "- so initial layers end up learning basic strokes, the next layer learn facial features such as eyes, nose , lips\n",
    "- nextlayers end up learning whole faces\n",
    "- Key-point: after each convolution/pooling the image size shrinks but filter size stays the same generally\n",
    "- common filter sizes are 3x3, 5x5 , 7x7\n",
    "- so they're generally much smaller than the input image\n",
    "- assume we start with 32x32 input, we used same mode convolution and pooling size is 2. and conv filters size 3x3.\n",
    "- 32x32 -> conv/ppoling -> 16x16 -> conv/ppoling -> 8x8 -> conv/ppoling -> 4x4\n",
    "- as filter size stays the same while image shrinking, the proportion of image that filter covers increases, so they find increasingly large patterns !!! THIS WHY CNN LEARN FEATURES HIERARCHICALLy\n",
    "- so in the first steps, it's looking for very tiny patterns, like simple edges and strokes\n",
    "- in the second step, it can look for patterns which take up more spaces in image, patterns like nose, eyes\n",
    "#### losing information:\n",
    "- we're obv using info by shrinking: we lose spatial information while shrinking\n",
    "- we don't care where features were found\n",
    "- but we haven't yet considered the number of feature maps!\n",
    "- while the size of image decreases, the number of feature map generally increases\n",
    "- Idc about where features found, only care where it was foıund, but I do care about multiple possible features that can be found\n",
    "- SO WE GAIN information in terms of what features were found\n",
    "### Hyperparameters\n",
    "- with conv: we need to choose filter size, number of feature maps, pool size, so on\n",
    "- luckily in CNN, convolutions are pretty standard\n",
    "- Guidelines:\n",
    "    - small filters relative to image size 3x3, 5x5, 7x7\n",
    "    - repeat conv -> pool\n",
    "    - increase number of feature maps\n",
    "    - read a lot of papers\n",
    "    \n",
    "### Alternative to pooling: Stride\n",
    "- researchers sometimes avoid pooling and do strided convolution instead\n",
    "- remember when we do pooling we had the stride paramater\n",
    "- in fact convolution has stride option as well\n",
    "- so multiply and add with a sliding window\n",
    "- when we have a convolution with stride parametr, stride parameter tells us how far apart each window should be\n",
    "- and intiutively : if you use a convolution of stride 2, the output image will be half the size of input image\n",
    "- you could get same size by applying a strided convolution instead of convolution followed by pooling\n",
    "\n",
    "### 2nd stage of CNN architecture - Feedforward NN part (Dense NN)\n",
    "- the shape of an image is not appropriate for a NN\n",
    "- output of convolution is 3-d : HxWxC with C showing number of fearure maps\n",
    "- Dense Layer expects a 1-D input vector - Hep mi yahu?\n",
    "- We already saw how this can work: Flatten()\n",
    "\n",
    "### Global Max Pooling\n",
    "- what if we have different image sizes? can CNN handle this?\n",
    "- yes, using a global max pooling layer\n",
    "- a simple flatten layer wouldn' work\n",
    "- an example to see why: \n",
    "    - suppose first image 32x32 and we do 4 strided convolutions with stride = 2\n",
    "    - output: 16x16 - 8x8 - 4x4 - 2x2\n",
    "    - suppose image size 64x64\n",
    "    - output: 4x4\n",
    "    - suppose final numebr of feature maps is 100\n",
    "    - when flatten : 400-d input vector for 32x32 image\n",
    "    - 1600-d input vector for 64x64 image\n",
    "    - NN cannot handle input vectors of different size\n",
    "#### how Global Max pooling work?\n",
    "- we always get an output 1x1xC regardless of H and W\n",
    "- In other words, it takes the max over the entire image along the spatial dimension for each feature map\n",
    "- glbal max pooling is a very extreme version of saying: Idc where the feature is found , just in the image\n",
    "- the only exception is when images are too small: then you'll get an error\n",
    "- if you start by images 2x2, you cannot downsample it by 2, ffour times\n",
    "- so you may encounter this problem if you try to add too many conv layers to your CNN (strided conv olursa herhaşde)\n",
    "\n",
    "### Summary\n",
    "- Step 1: \n",
    "    - conv > pool > conv > pool > conv...\n",
    "    - strided conv < strided conv > ...\n",
    "- Step 2:\n",
    "    - Flatten()\n",
    "    - GlobalMaxPooling2D()\n",
    "- Step 3:\n",
    "    - Dense > Dense > Dense...\n",
    "    \n",
    "#### as usual the activation and number of nodes in the final layer depends on the task\n",
    "- Regression : 1 output node, activation: none\n",
    "- Binary : 1 output node, activation: sigmoid\n",
    "- multiclass : k output nodes, activation: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
