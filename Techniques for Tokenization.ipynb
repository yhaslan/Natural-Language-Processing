{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62ddacb",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "### 1- Countvectorizer class from sklearn.feature_extraction.text\n",
    "- removes punctuation\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa584e56",
   "metadata": {},
   "source": [
    "- CountVectorizer(lowercase= True) default\n",
    "- CountVectorizer(strip_accents = True) for accents by default None, Does not strip accents from the text.\n",
    "- CountVectorizer(analyzer = \"word\") by default you can change it to \"char\"\n",
    "- stopwords: CountVectorizer(stop_words = \"english\")\n",
    "- CountVectorizer(stop_words = \"list of user defined words\") if you work with other words \n",
    "- mesela bu etapta nltk'dan indirdiğimiz list'i CountVectorizer ın içine yazabilir miyiz?\n",
    "- default: CountVectorizer() tokenize by string split\n",
    "- CountVectorizer(tokenizer = ) içine seçtiğimiz başka bi tokenizer koyabiliriz\n",
    "- max_feature argüman tells us the length of vector of words, kaç kolon olcak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43faa189",
   "metadata": {},
   "source": [
    "### 2- simplest possible tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd2c232e",
   "metadata": {},
   "source": [
    "def simple_tokenizer(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3154deb",
   "metadata": {},
   "source": [
    "- mesela bunu CountVectorizer(tokenizer = simple_tokenizer) ile count'a çevirebiliriz\n",
    "- ya da tokenize edip sonra count vectorizer'a sokabikiriz, örn: tokenize edip tf-idf'e sokmuşuz Recommender Exercise Prompt'ta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3db76b",
   "metadata": {},
   "source": [
    "### 3- word_tokenize class from nltk library"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39abe6d5",
   "metadata": {},
   "source": [
    "ex: words = word_tokenize(doc.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792e84c",
   "metadata": {},
   "source": [
    "- we'll use nltk's word_tokenize() hoca appropraite bulmuş as it keeps punctuation as separate token\n",
    "- we did so in article spinning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2e0d0",
   "metadata": {},
   "source": [
    "### other method for stopwords:\n",
    "- nltk'den "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5effc78",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "stopwords.words(\"german\")\n",
    "\n",
    "- kirill bu aşamadan sonra: \n",
    "all_stopwords = stopwords.words(\"english\")\n",
    "- if you wanna modify:\n",
    "all_stopwords.remove(\"not\")\n",
    "\n",
    "review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "review = ' '.join(review)\n",
    "corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee30bd7",
   "metadata": {},
   "source": [
    "- bu line kafa karıştırıcı gibi ama aslında değil, word stopword listesinde değilse stem et ve yapıştır demek ama stopword olanları değiştirmeden tut demek değil, review'i artık yeni bi var gibi düşün\n",
    "- sonra CountVectorizer'a direkt corpus'u sokuyo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab0690",
   "metadata": {},
   "source": [
    "### or tf-idf for not having to deal with stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1e6f59f",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b77ab",
   "metadata": {},
   "source": [
    "- bcz we cannot know if our stopword list is correct\n",
    "- TfdifVectorizer(max_features) gene belirlenebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace78a9",
   "metadata": {},
   "source": [
    "### another way to remove punctuation:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7908728b",
   "metadata": {},
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation)) we used for generating poems\n",
    "for line in open('robert_frost.txt'):\n",
    "    tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "    \n",
    "   ##lstrip, rstrip, strip Bu metodlar bir dizenin sol ve/veya sağ tarafındaki boş karakterleri kaldırır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced8d7c",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "#### Stemming:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47646482",
   "metadata": {},
   "source": [
    "from nltk import PorterStemmer\n",
    "ps = PorterStemmer ()\n",
    "ps.stem(word)\n",
    "\n",
    "#Kirill önce stem'li halini bi corpus'a kaydedip sonra count vectorize etmişti:\n",
    "review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "review = ' '.join(review)\n",
    "corpus.append(review)\n",
    "\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#the fit method will just take all the words and transform will transform them to the columns\n",
    "#it should be 2d array to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd96b1",
   "metadata": {},
   "source": [
    "- Kirill Gaussian modelde train edebilmek için X'i 2D array'e çeviriyo\n",
    "- ama CountVectorizer with Sklean egzerisizinde Lazy Programmer öyle bi şey yapmamış, niye fark ne?\n",
    "- sanırım kirill corpus'u append ile yaratırken listeye çeviriyo sonra o yüzden toarray() ile 2d-array'e çevirmesi lazım, 2d array is same as sparse matrix galiba"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9542b31b",
   "metadata": {},
   "source": [
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def call(self,doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        return(self.porter.stem(t) for t in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dcbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer= StemTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f146e44",
   "metadata": {},
   "source": [
    "#### Lemmatization:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5631f8c4",
   "metadata": {},
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(word)\n",
    "lemmatizer.lemmatize(\"going\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbaec39",
   "metadata": {},
   "source": [
    "- BUT!!!: POS tags not compatible with WordNetLemmatizer\n",
    "- first you need to map tags one format to other"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c2a3034",
   "metadata": {},
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57e9d558",
   "metadata": {},
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "  if treebank_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd263491",
   "metadata": {},
   "source": [
    "class LemmaTokenizer:\n",
    "  def __init__(self):\n",
    "    self.wnl = WordNetLemmatizer()\n",
    "  def __call__(self, doc):\n",
    "    tokens = word_tokenize(doc) #this converts doc to tokens, like a fancier version of string split\n",
    "    words_and_tags = nltk.pos_tag(tokens) #this obtains pos tags, returns a tuple\n",
    "    return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) \\\n",
    "            for word, tag in words_and_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer= LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ed598",
   "metadata": {},
   "source": [
    "### Kirill cleanin için re diye bi package:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9e1fe69",
   "metadata": {},
   "source": [
    "re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d0846",
   "metadata": {},
   "source": [
    "Here's an explanation of what the code does:\n",
    "- re.sub() is a function that performs regular expression-based substitution in a string.\n",
    "- The first argument [^a-zA-Z] is a regular expression pattern that matches any character that is not a lowercase or uppercase letter. The ^ symbol within square brackets negates the character class, so [^a-zA-Z] matches any character that is not an alphabetic letter.\n",
    "- The second argument ' ' specifies the replacement string. In this case, it replaces any character matched by the pattern with a space.\n",
    "- dataset['Review'][i] represents the input string on which the substitution operation is applied. It seems to be accessing the 'Review' column of a dataset or dataframe at index i.\n",
    "- In summary, this code is likely used to remove or replace any non-alphabetic characters (such as numbers, symbols, or punctuation) in the 'Review' string with spaces. This operation can be useful for text preprocessing tasks where you want to focus only on alphabetic characters or remove unwanted characters from the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf690c3c",
   "metadata": {},
   "source": [
    "### SCORE:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58b349a1",
   "metadata": {},
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "\n",
    "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
    "print(\"test score:\", model.score(Xtest, Ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
