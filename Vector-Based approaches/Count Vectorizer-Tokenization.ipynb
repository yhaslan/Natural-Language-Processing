{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e0830d",
   "metadata": {},
   "source": [
    "### Count Vectorizer class in Scikit Learn\n",
    "\n",
    "How to treat different situations?\n",
    "\n",
    "a- Punctuation\n",
    "\n",
    "b- Casing\n",
    "\n",
    "c- Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1e89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count Vectorizer class from Sklearn library to countin the bag of words model\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765eec9",
   "metadata": {},
   "source": [
    "Punctuation : \"I hate cats.\" not the same as \"I hate cats?\" in sentiment analysis\n",
    "\n",
    "one way is to tokenize as [\"I\",\"hate\", \"cats\", \"?\"]\n",
    "by default Sklearn CV ignores the punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59727815",
   "metadata": {},
   "source": [
    "#### Casing\n",
    "cat or Cat, do I want them to be treated the same?\n",
    "\n",
    "in python string.lower()\n",
    "\n",
    "in sklearn\n",
    "\n",
    "CV(lowercase = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557af5bf",
   "metadata": {},
   "source": [
    "#### Accents : in english usually ignored\n",
    "\n",
    "naive or naive (with double dot)\n",
    "\n",
    "résumé or resume\n",
    "\n",
    "do I want them to be treated same\n",
    "\n",
    "CV(strip_accents = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fbf13",
   "metadata": {},
   "source": [
    "### character-based tokenizations: when our tokens are not full words\n",
    "\n",
    "see better the pros cons of word-based token. when learn deep learning\n",
    "\n",
    "word-based tokenization, vectors length 1 million, 1 million embeddings\n",
    "\n",
    "in probability distributionn, in deep learning it will require a 1 million size weight matrix\n",
    "\n",
    "Pro: words have meaning and contain a lot info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa86517",
   "metadata": {},
   "source": [
    "### subword-based tokenization\n",
    "\n",
    "a middle ground btw ch-based and word-based\n",
    "\n",
    "ex: walking -- walk + ing\n",
    "\n",
    "if we don't do that walk and walking will be treated soo differently that they will be similar distance to each other to the distance they are to \"tree\", the model will learn no relationship btw them\n",
    "\n",
    "Do we want our model to learn walk, walks, walked, walking independently or do we want in a shared representation\n",
    "\n",
    "#### we made a strong case for subword-based token.  but we won't see this again until Transformers\n",
    "\n",
    "although şt's a good idea in theory, not necessarily always good to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer(analyzer = \"word\") #DEFAULT\n",
    "CountVectorizer(analyzer = \"char\")\n",
    "\n",
    "#FOR CHAR-BASED TOKENIZATION A STRING IS ALREADY TOKENIZED, IN CASE YOU IMPLEMENT TOKENIZATION YOURSELF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f7fae",
   "metadata": {},
   "source": [
    "### TOKENIZATION DOES NOT INVOLVE ML\n",
    "\n",
    "even when you do yourself: it's string split\n",
    "\n",
    "when you need to remove accents or uppercase = it's just char mapping to another char\n",
    "\n",
    "when you check punctuation and subword tokenization : anlatmadı\n",
    "\n",
    "but it's basic programming and not machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2493b70",
   "metadata": {},
   "source": [
    "### STOPWORDS:\n",
    "\n",
    "CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "CountVectorizer(stop_words = \"list of user defined words\") #if you're working in another language\n",
    "\n",
    "CountVectorizer(stop_words = None) by default\n",
    "\n",
    "#### If you wanna do NLP in another language, one possibility is to use NLTK stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0247be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "stopwords.words(\"german\")\n",
    "\n",
    "#many languages included, see on webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f2b3f",
   "metadata": {},
   "source": [
    "### STEMMING AND LEMMATIZATION\n",
    "\n",
    "two methods to convert words to their roots\n",
    "\n",
    "- stemming : very crude, just chps the end of the word\n",
    "- lemmatization: more sophisticaded, use actual rules of lang, the root word will be returned , root also known as lemma\n",
    "\n",
    "#### stemming\n",
    "- bosses => boss\n",
    "- replacement=> replac (not a real word)\n",
    "- most populer PorterStemmer Class of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baede338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem(\"walking\") #returns \"walk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f15eac",
   "metadata": {},
   "source": [
    "#### lemmatization\n",
    "- think as a lokkup table, a dictionary\n",
    "- stemming : better => better\n",
    "- lemmatizaion : better => good, was => be, mice => mouse\n",
    "- appears in many py libraries: NLTK, SpaCy and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"mice\") #returns mouse\n",
    "\n",
    "lemmatizer.lemmatize(\"going\") #returns going\n",
    "lemmatizer.lemmatize(\"going\", pos = wordnet.VERB) #returns go #pos refers to parts of speech\n",
    "\n",
    "#pos argument not required but reccomemende to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddae33",
   "metadata": {},
   "source": [
    "#### soru: yani donald trump demands a following vs . i was following\n",
    "\n",
    "- bu durumda pos hangisinin verb olup suffix in atılacağını hangisinin noun olup kalacağını anlayabiliyo mu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1cd3f",
   "metadata": {},
   "source": [
    "### To properly use the lemmatizer:\n",
    "- you need first POS tagging\n",
    "- NLTK can do this\n",
    "- BUT!!!: POS tags not compatible with WordNetLemmatizer\n",
    "- first you need to map tags one format to other\n",
    "\n",
    "#### tam anlamadım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aace72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
