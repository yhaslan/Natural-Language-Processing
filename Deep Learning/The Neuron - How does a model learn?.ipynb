{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c3c53d",
   "metadata": {},
   "source": [
    "#### In this lecture, we'll look at the models we learnt more in-depth\n",
    "- Linear Regression for Regression\n",
    "- Logistic Regression for Classification\n",
    "\n",
    "### Linear Regression\n",
    "- a way to think of the weight is how much matters an input (feature importance)\n",
    "\n",
    "### neuron - biology\n",
    "- a single neuron takes input form a lot different places, from dendrites of other neurons\n",
    "- now this neuron has to decide to whether or not pass the signal to the outgoing neurons\n",
    "- It does this in a very similar way to linear and logistic regression\n",
    "- It sums up all the incoming signals\n",
    "- and then this summmation becomes the output signal that goes to ongoings neurons\n",
    "- not all neuron connections are equal, some neuron connections are weak, some are strong\n",
    "- some connections are excitatory (+) some are inhibitory(-)\n",
    "- these are just like the weights in a regression model\n",
    "- action potential: the signal that got passed on neuron has a special name. if you measure the electrical potential over time at a particular neuron, you would see a signal like this. action potentials can be thought as a binary outcome just like logistic regression\n",
    "- so a neuron is more like a logistic reg than linear reg.\n",
    "- how action potential works? we sum up all the incoming signal from all sending neurons. if the electrical potential of this some is greater than some threshold, then an action potential will propogate thru the receiving neuron\n",
    "- in biology it's called all or nothing principle: you get a spike or you don't\n",
    "\n",
    "### so neuron- DL:\n",
    "- each x_i is an input from some incoming neuron\n",
    "- each w_i weight denotes how strong the connection is\n",
    "- each sign assigned to w_i will tell if that input will work exitatory or inhibatory, meaning whether it positively or negatively influences the action potential\n",
    "- weighted sum of each x_i , added also a bias term, passes thru the sigmoid function\n",
    "- once the sigmoid funciton is rounded, we get either one or zero, telling us whether or not an actual potential should occur\n",
    "\n",
    "#### This is the inspiration behind Deep Learning, unlike other ML models like SVMs, trees that are just pure maths\n",
    "#### DL is inspired directly from biology, what differentiates it from other forms of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002bd27",
   "metadata": {},
   "source": [
    "## How does a model learn?\n",
    "- wet're gonna define an error function\n",
    "- MSE : mean squared error (or average sum of squares) \n",
    "- sanırım residual some of squares'ın average hali\n",
    "- error = cost = loss\n",
    "### Minimizing the cost\n",
    "- turn to calculus, min-max nasıl bulunuyodu? tangent'ın türevinin sıfır oldupu noktada\n",
    "- what if there is more than one variable?\n",
    "- the multidimensional equivalent of derivative is gradient\n",
    "- to solve for model parameters, we find the gradient and set it to 0.\n",
    "- Gradient is just a vector or tensor of partial derivatives.\n",
    "\n",
    "- In this course we won't calculate gradient manually but he has many other courses doing that.\n",
    "- TensorFlow already does that for us using a process called automatic differentiation\n",
    "    - tensorflow will automatically find all your weights\n",
    "    - use them to train your model\n",
    "    \n",
    "### Gradient Descent\n",
    "- you might be wondering: if all to do was to find gradient and set to zero, why in the previous courses we did all those itertaive processes?\n",
    "- recall when we specified epoch and checked for convergence?\n",
    "- most of the time, it's not actually possible to solve the equation that set gradient = 0\n",
    "- Linear reg is exception\n",
    "- For logistic reg and other models onward we cannot solve it, we need another approach\n",
    "- that's called Gradient Descent\n",
    "- start from a randomly initialized point for both w and b\n",
    "- then we compute the gradient for our loss wtr to w and b\n",
    "- we then take small steps to this direction to update value w and b on each iteration (epochs)\n",
    "#### This is exactly what's going on inside Keras fit function.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f97ff1f0",
   "metadata": {},
   "source": [
    "w,b = randomly initialized\n",
    "for epoch in range(epochs) : \n",
    "    w = w - n * gradient_w J\n",
    "    b = b - n * gradient_b J\n",
    "    \n",
    "    n: specifies the learning rate, the step size we're taking, how fast or slow we wanna train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20dce9",
   "metadata": {},
   "source": [
    "#### It's important to set this value right otherwise it won't get good results even though it's a good model\n",
    "- there is no simple way to decide on learning rate\n",
    "- This is called a hyperparameter whereas w and b called parameters\n",
    "- mostly by trial and error\n",
    "- intution get by seeing many examples\n",
    "- in fact no hyperparameters are chosen directly, they're more trial and error and intuition gained\n",
    "- the only way to know is check the loss per iteration, means you need to spend some time trainig\n",
    "- normally try powers of 10: 0.1, 0.01, 0.001..\n",
    "- grafiği ipad'e çizdim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1aa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
