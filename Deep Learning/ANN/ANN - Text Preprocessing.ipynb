{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325d2551",
   "metadata": {},
   "source": [
    "- This lecture is about doing text preprocessing using the Keras API.\n",
    "- we're just using a library for something we've already been doing\n",
    "- just Keras makes it easier to do the steps\n",
    "- in fact API is so nice that sometimes PyTorch users use Keras Preprocesing also cause PyTorch library is very bad\n",
    "- easier but less flexible (ex: index 0 reserved for padding so you won't be able to assign that to a word)\n",
    "- This makes the embedding matrix somewhat inefficient bcz there'll be a useless row\n",
    "\n",
    "# Text Preprocessing Keras API Code Preparation\n",
    "\n",
    "### Remember the steps:\n",
    "- Tokenize the text (convert each doc in corpus into a list of tokens, each token may be a whole word, part of the word or just a character)\n",
    "- assign integer ID to each unique token , bcz integer will represent position in a feature vector\n",
    "- From now on, we won't be using tf-idf (but word embeddings) but we still need to map tokens to integers\n",
    "- in most of DL, we won't be using bag of words but instead we'll work with full sequences\n",
    "- NEW STEP: after mapping each token to an integer, convert the document into a sequence of integers\n",
    "ex: I like cats -> [407,634,27] a list\n",
    "- why helpful? Integers take up less memory than strings\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f024055e",
   "metadata": {},
   "source": [
    "from tenserflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_text(train_text) #like fit method on sklearn\n",
    "sequences_train = tokenizer.texts_to_sequences(train_text)  #like transform method on sklearn\n",
    "sequences_test = tokenizer.texts_to_sequences(test_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c08ae1",
   "metadata": {},
   "source": [
    "- as usual you only fit on train set, transform on both\n",
    "- you should pass the documents as a list of strings to this code OR alternatively a list of tokens.\n",
    "- [\"I am doc 1\", \"I am doc 2\", ... ]\n",
    "or\n",
    "- [[\"I\",\"am\", \"doc\", \"1\"], [\"I\",\"am\", \"doc\", \"2\"], ...]\n",
    "- so although name says tokenizer, it does many other things: it maps, it converts each doc into an integer list and it even works when your input's already tokenized so a bit weird to call it tokenizer\n",
    "\n",
    "### Tokenizer output:\n",
    "- sentences= [\"I like eggs and ham.\", \"I love chocolates and bunnies.\", \"I hate onions\"] becomes \n",
    "- sequences = [[1,2,3,4,5], [1,6,7,4,8], [1,9,10]]\n",
    "- how do you know which word goes with which integer?\n",
    "- Answer: tokenizer.word_index\n",
    "\n",
    "### Constructor arguments:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5413fdbd",
   "metadata": {},
   "source": [
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words = None, #allows you to specify max number of words you wanna keep\n",
    "    filters = '!\"#$.....' , #characters to ignore, default da bırak\n",
    "    lower = True, \n",
    "    split = ' ', \n",
    "    char_level = False,\n",
    "    oov_token = None #out-of-vocabulary token, i.e. unique words from test set, if not specified will be removed, if you set this argument, it will be stored here. ee nasıl set etcem?\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e415af",
   "metadata": {},
   "source": [
    "### Stemming? Lemmatization? Stopwords?\n",
    "- you normally don't want these things in DL\n",
    "- ex: suppose you're trying to generate text, it will be difficult if you couldn't generate stopwords\n",
    "- or if you do translation, running and I ran should be translated diffrently, so not good to have lemmatization either\n",
    "\n",
    "### Text to Matrix\n",
    "- this is another method he wanna discuss as a bonus\n",
    "- this allows you to convert your text to tfidf, count vectors or binary vectors as we previously done in the course\n",
    "- not gonna be neededin DL, we actually wanna keep our text as a sequence but just know that they exist\n",
    "- bu kadar bahsetti :D\n",
    "\n",
    "## Looking Ahead: Padding\n",
    "- unpadded sequences = [[1,2,3,4,5], [1,6,7,4,8], [1,9,10}]]\n",
    "- padded sequences = [[1,2,3,4,5], [1,6,7,4,8], [0,0,1,9,10}]]\n",
    "- why padding required is bcz in libraries like Tensorflow sequences all must have the same length\n",
    "- there is no jagged arrays in numpy but in text we have different lengths, not all sentences or all documents have the same length\n",
    "- you won't see why this is true until CNN. but trust it for now"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60ed2886",
   "metadata": {},
   "source": [
    "padded_seq = pad_sequences(unpadded_seq) #a function which is also part of keras api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd9be0",
   "metadata": {},
   "source": [
    "- looking ahead bit again, we actually typically add zeroes at the front, since RNNs have trouble remembering things from too far past, so it's usually more useful to make padding to the front- but we'll explore other options too- and keep the most useful information near the end\n",
    "- e o zaman sıfır indexli kelime olmaması iyi oldu, sıfırla padding yapabiliyoz\n",
    "\n",
    "### Shapes\n",
    "- as usual, it's good to think in terms of shapes\n",
    "- a numpy array or a tensorflow tensor contaninin padded sequences of integers representing sentences will be shaped NxT\n",
    "- N-number of documents T-document length\n",
    "\n",
    "## Exercise - Text Preprocessing with Tensorflow\n",
    "- bunun notebook'unu websitesinde bulamadım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e8d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 19:17:18.692092: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b122af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#yukarda uzun yazmak yerine tf.keras diye yazınca error : No module named 'tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8580637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a simple text\n",
    "sentences= [\"I like eggs and ham.\", \n",
    "            \"I love chocolates and bunnies.\", \n",
    "            \"I hate onions.\"\n",
    "           ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac04c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 2, 5], [1, 6, 7, 2, 8], [1, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 20000 #probably good enough, 3000 will cover 95% words already\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26777558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'and': 2,\n",
       " 'like': 3,\n",
       " 'eggs': 4,\n",
       " 'ham': 5,\n",
       " 'love': 6,\n",
       " 'chocolates': 7,\n",
       " 'bunnies': 8,\n",
       " 'hate': 9,\n",
       " 'onions': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get word to index mapping\n",
    "tokenizer.word_index\n",
    "\n",
    "#as you see zero is reserved for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40b3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4  2  5]\n",
      " [ 1  6  7  2  8]\n",
      " [ 0  0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "#use the defaults ? we wanna pad sequences with all the default values, baka nacaktık ki?\n",
    "data = pad_sequences(sequences)\n",
    "print(data)\n",
    "\n",
    "#default appears to set maximmum sequence length to maximum sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "734ca774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4  2  5]\n",
      " [ 1  6  7  2  8]\n",
      " [ 0  0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 5\n",
    "data = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH ) #we can pass explicitly the max len\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70dc549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4  2  5]\n",
      " [ 1  6  7  2  8]\n",
      " [ 1  9 10  0  0]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH, padding ='post') ##padding i sona yaptık\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "940b7345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  3  4  2  5]\n",
      " [ 0  1  6  7  2  8]\n",
      " [ 0  0  0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen = 6 ) #too much padding\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7256d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  2  5]\n",
      " [ 6  7  2  8]\n",
      " [ 0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen = 4 ) #too little padding == truncation, cut off the begining of sentences\n",
    "#makes sense bcz NN typically pay more attention to the final values of sequence anywya\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df6da707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4  2]\n",
      " [ 1  6  7  2]\n",
      " [ 0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen = 4, truncating ='post' ) #truncating'i post yaparak sondan kesyik\n",
    "print(data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb06f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
