{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ad5d76",
   "metadata": {},
   "source": [
    "## TF- IDF to improve the count vectorizer\n",
    "\n",
    "popular for document retriveal and textmining\n",
    "\n",
    "#### what was wrong with count vectorizer? \n",
    "- we don't want stopwords as they're not useful for nlp\n",
    "- but how do we know our list for stepwords is correct? we don't. they can be document specific\n",
    "\n",
    "### TF IDF scale down the words based on how many documents they appear in: \n",
    "Term Frequency - Inverse Document Frequency = Term Frequency / Inverse Document Frequency (intuition not the formula)\n",
    "\n",
    "Formülü detaylı ipad'de anlattım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4dffc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/9bnp6m5d7db0pky3235l4wgh0000gn/T/ipykernel_1599/1213340805.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "Xtrain = tfidf.fit_transform(input_data)\n",
    "Xtest = tfidf.transform(test_data)\n",
    "\n",
    "#note: arguments exist for stopwrods, tokenizer, strip accents etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbae5b",
   "metadata": {},
   "source": [
    "### Term Frequency variations\n",
    "- Binary (1 if word appears in doc, 0 if not)\n",
    "- Normalize the count (sometimes this is default) : divide the sum to the number of all terms in a document\n",
    "- take the log of 1 + count(t,d) #to reduce the effect of extreme values\n",
    "\n",
    "### Inverse Document Frequency variations\n",
    "- Smooth IDF : log( N / (N(t)+1)) +1 #prevents us from value of zero\n",
    "- IDF max: instead of using N, use maximum term count from the same document, ynai tüm dokümanlara değil, aynı dokümanda en çok çıkan kelimenin çıkma sayısını paya yazıyo\n",
    "- Probablistic IDF: log ( N- N(t)) / N(t)) => log odds also aka logit\n",
    "\n",
    "### Another variation: Noralizing entire TF-IDF vector\n",
    "take the vector and divide it by L2 norm (by its length), this makes the length of every tfidf vector 1.\n",
    "- advantage: ranking by euclidian distance and cosine distance will yield the same result\n",
    "- unlike count vectorizer, tfidfvectorizer in sklearn supports this type of normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca485350",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer(norm=\"l2\") #for normalization\n",
    "#l2 is a default so if you don't put anything your tfidf vector will be normalized to have unit length\n",
    "#else you can write norm=\"l1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e315e",
   "metadata": {},
   "source": [
    "## Build TF-IDF from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef884eda",
   "metadata": {},
   "source": [
    "Here we'll build from scratch\n",
    "\n",
    "then take random docs fro dataset and check their tf-idf values\n",
    "\n",
    "the one with the high tf-idf should have two traits:\n",
    "- they should appear a lot in the doc\n",
    "- they should be unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3367f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'bbc_text_cls.csv' already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification\n",
    "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967cf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691e73cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bbc_text_cls.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2782f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2idx mapping\n",
    "#convert docs into sequences of indices\n",
    "\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "tokenized_docs = []\n",
    "\n",
    "for doc in df[\"text\"]:\n",
    "    words = word_tokenize(doc.lower())\n",
    "    doc_as_int = []\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx #add a key-value pair to the dictionary\n",
    "            idx += 1\n",
    "            \n",
    "        doc_as_int.append(word2idx[word])\n",
    "#şimdi alt satırdaki dokğmana geçiyoruz\n",
    "    tokenized_docs.append(doc_as_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7716fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a reverse mapping as well\n",
    "#itwill iterate through our word2idx dictionary and simply switch the value and the key\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "\n",
    "#yet this is somwhat inefficient bcz we're using a dictionay structure, even tho indices are values from zero up to the\n",
    "#vocab size\n",
    "\n",
    "#you may find a smarter way and store it in a list instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd9a35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad',\n",
       " 'sales',\n",
       " 'boost',\n",
       " 'time',\n",
       " 'warner',\n",
       " 'profit',\n",
       " 'quarterly',\n",
       " 'profits',\n",
       " 'at',\n",
       " 'us',\n",
       " 'media',\n",
       " 'giant',\n",
       " 'timewarner',\n",
       " 'jumped',\n",
       " '76',\n",
       " '%',\n",
       " 'to',\n",
       " '$',\n",
       " '1.13bn',\n",
       " '(',\n",
       " '£600m',\n",
       " ')',\n",
       " 'for',\n",
       " 'the',\n",
       " 'three',\n",
       " 'months',\n",
       " 'december',\n",
       " ',',\n",
       " 'from',\n",
       " '639m',\n",
       " 'year-earlier',\n",
       " '.',\n",
       " 'firm',\n",
       " 'which',\n",
       " 'is',\n",
       " 'now',\n",
       " 'one',\n",
       " 'of',\n",
       " 'biggest',\n",
       " 'investors',\n",
       " 'in',\n",
       " 'google',\n",
       " 'benefited',\n",
       " 'high-speed',\n",
       " 'internet',\n",
       " 'connections',\n",
       " 'and',\n",
       " 'higher',\n",
       " 'advert',\n",
       " 'said',\n",
       " 'fourth',\n",
       " 'quarter',\n",
       " 'rose',\n",
       " '2',\n",
       " '11.1bn',\n",
       " '10.9bn',\n",
       " 'its',\n",
       " 'were',\n",
       " 'buoyed',\n",
       " 'by',\n",
       " 'one-off',\n",
       " 'gains',\n",
       " 'offset',\n",
       " 'a',\n",
       " 'dip',\n",
       " 'bros',\n",
       " 'less',\n",
       " 'users',\n",
       " 'aol',\n",
       " 'on',\n",
       " 'friday',\n",
       " 'that',\n",
       " 'it',\n",
       " 'owns',\n",
       " '8',\n",
       " 'search-engine',\n",
       " 'but',\n",
       " 'own',\n",
       " 'business',\n",
       " 'had',\n",
       " 'has',\n",
       " 'mixed',\n",
       " 'fortunes',\n",
       " 'lost',\n",
       " '464,000',\n",
       " 'subscribers',\n",
       " 'lower',\n",
       " 'than',\n",
       " 'preceding',\n",
       " 'quarters',\n",
       " 'however',\n",
       " 'company',\n",
       " \"'s\",\n",
       " 'underlying',\n",
       " 'before',\n",
       " 'exceptional',\n",
       " 'items',\n",
       " 'back',\n",
       " 'stronger',\n",
       " 'advertising',\n",
       " 'revenues',\n",
       " 'hopes',\n",
       " 'increase',\n",
       " 'offering',\n",
       " 'online',\n",
       " 'service',\n",
       " 'free',\n",
       " 'customers',\n",
       " 'will',\n",
       " 'try',\n",
       " 'sign',\n",
       " 'up',\n",
       " 'existing',\n",
       " 'broadband',\n",
       " 'also',\n",
       " 'restate',\n",
       " '2000',\n",
       " '2003',\n",
       " 'results',\n",
       " 'following',\n",
       " 'probe',\n",
       " 'securities',\n",
       " 'exchange',\n",
       " 'commission',\n",
       " 'sec',\n",
       " 'close',\n",
       " 'concluding',\n",
       " 'slightly',\n",
       " 'better',\n",
       " 'analysts',\n",
       " \"'\",\n",
       " 'expectations',\n",
       " 'film',\n",
       " 'division',\n",
       " 'saw',\n",
       " 'slump',\n",
       " '27',\n",
       " '284m',\n",
       " 'helped',\n",
       " 'box-office',\n",
       " 'flops',\n",
       " 'alexander',\n",
       " 'catwoman',\n",
       " 'sharp',\n",
       " 'contrast',\n",
       " 'when',\n",
       " 'third',\n",
       " 'final',\n",
       " 'lord',\n",
       " 'rings',\n",
       " 'trilogy',\n",
       " 'boosted',\n",
       " 'full-year',\n",
       " 'posted',\n",
       " '3.36bn',\n",
       " 'performance',\n",
       " 'while',\n",
       " 'grew',\n",
       " '6.4',\n",
       " '42.09bn',\n",
       " '``',\n",
       " 'our',\n",
       " 'financial',\n",
       " 'was',\n",
       " 'strong',\n",
       " 'meeting',\n",
       " 'or',\n",
       " 'exceeding',\n",
       " 'all',\n",
       " 'objectives',\n",
       " 'greatly',\n",
       " 'enhancing',\n",
       " 'flexibility',\n",
       " \"''\",\n",
       " 'chairman',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'richard',\n",
       " 'parsons',\n",
       " '2005',\n",
       " 'projecting',\n",
       " 'operating',\n",
       " 'earnings',\n",
       " 'growth',\n",
       " 'around',\n",
       " '5',\n",
       " 'expects',\n",
       " 'revenue',\n",
       " 'wider',\n",
       " 'margins',\n",
       " 'accounts',\n",
       " 'as',\n",
       " 'part',\n",
       " 'efforts',\n",
       " 'resolve',\n",
       " 'an',\n",
       " 'inquiry',\n",
       " 'into',\n",
       " 'market',\n",
       " 'regulators',\n",
       " 'already',\n",
       " 'offered',\n",
       " 'pay',\n",
       " '300m',\n",
       " 'settle',\n",
       " 'charges',\n",
       " 'deal',\n",
       " 'under',\n",
       " 'review',\n",
       " 'unable',\n",
       " 'estimate',\n",
       " 'amount',\n",
       " 'needed',\n",
       " 'set',\n",
       " 'aside',\n",
       " 'legal',\n",
       " 'reserves',\n",
       " 'previously',\n",
       " '500m',\n",
       " 'intends',\n",
       " 'adjust',\n",
       " 'way',\n",
       " 'with',\n",
       " 'german',\n",
       " 'music',\n",
       " 'publisher',\n",
       " 'bertelsmann',\n",
       " 'purchase',\n",
       " 'stake',\n",
       " 'europe',\n",
       " 'reported',\n",
       " 'book',\n",
       " 'sale',\n",
       " 'loss',\n",
       " 'value',\n",
       " 'dollar',\n",
       " 'greenspan',\n",
       " 'speech',\n",
       " 'hit',\n",
       " 'highest',\n",
       " 'level',\n",
       " 'against',\n",
       " 'euro',\n",
       " 'almost',\n",
       " 'after',\n",
       " 'federal',\n",
       " 'reserve',\n",
       " 'head',\n",
       " 'trade',\n",
       " 'deficit',\n",
       " 'stabilise',\n",
       " 'alan',\n",
       " 'highlighted',\n",
       " 'government',\n",
       " 'willingness',\n",
       " 'curb',\n",
       " 'spending',\n",
       " 'rising',\n",
       " 'household',\n",
       " 'savings',\n",
       " 'factors',\n",
       " 'may',\n",
       " 'help',\n",
       " 'reduce',\n",
       " 'late',\n",
       " 'trading',\n",
       " 'new',\n",
       " 'york',\n",
       " 'reached',\n",
       " '1.2871',\n",
       " '1.2974',\n",
       " 'thursday',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'greenback',\n",
       " 'recent',\n",
       " 'mr',\n",
       " 'london',\n",
       " 'ahead',\n",
       " 'g7',\n",
       " 'finance',\n",
       " 'ministers',\n",
       " 'sent',\n",
       " 'earlier',\n",
       " 'tumbled',\n",
       " 'worse-than-expected',\n",
       " 'jobs',\n",
       " 'data',\n",
       " 'i',\n",
       " 'think',\n",
       " 'taking',\n",
       " 'much',\n",
       " 'more',\n",
       " 'sanguine',\n",
       " 'view',\n",
       " 'current',\n",
       " 'account',\n",
       " 'he',\n",
       " 'taken',\n",
       " 'some',\n",
       " 'robert',\n",
       " 'sinche',\n",
       " 'currency',\n",
       " 'strategy',\n",
       " 'bank',\n",
       " 'america',\n",
       " 'longer-term',\n",
       " 'laying',\n",
       " 'out',\n",
       " 'conditions',\n",
       " 'can',\n",
       " 'improve',\n",
       " 'this',\n",
       " 'year',\n",
       " 'next',\n",
       " 'worries',\n",
       " 'china',\n",
       " 'do',\n",
       " 'remain',\n",
       " 'remains',\n",
       " 'pegged',\n",
       " 'falls',\n",
       " 'have',\n",
       " 'therefore',\n",
       " 'made',\n",
       " 'chinese',\n",
       " 'export',\n",
       " 'prices',\n",
       " 'highly',\n",
       " 'competitive',\n",
       " 'calls',\n",
       " 'shift',\n",
       " 'beijing',\n",
       " 'policy',\n",
       " 'fallen',\n",
       " 'deaf',\n",
       " 'ears',\n",
       " 'despite',\n",
       " 'comments',\n",
       " 'major',\n",
       " 'newspaper',\n",
       " 'ripe',\n",
       " 'loosening',\n",
       " 'peg',\n",
       " 'thought',\n",
       " 'unlikely',\n",
       " 'produce',\n",
       " 'any',\n",
       " 'meaningful',\n",
       " 'movement',\n",
       " 'meantime',\n",
       " 'decision',\n",
       " 'february',\n",
       " 'interest',\n",
       " 'rates',\n",
       " 'point',\n",
       " '-',\n",
       " 'sixth',\n",
       " 'such',\n",
       " 'move',\n",
       " 'many',\n",
       " 'opened',\n",
       " 'differential',\n",
       " 'european',\n",
       " 'half-point',\n",
       " 'window',\n",
       " 'believe',\n",
       " 'could',\n",
       " 'be',\n",
       " 'enough',\n",
       " 'keep',\n",
       " 'assets',\n",
       " 'looking',\n",
       " 'attractive',\n",
       " 'prop',\n",
       " 'partly',\n",
       " 'been',\n",
       " 'result',\n",
       " 'big',\n",
       " 'budget',\n",
       " 'deficits',\n",
       " 'well',\n",
       " 'yawning',\n",
       " 'gap',\n",
       " 'both',\n",
       " 'need',\n",
       " 'funded',\n",
       " 'buying',\n",
       " 'bonds',\n",
       " 'foreign',\n",
       " 'firms',\n",
       " 'governments',\n",
       " 'white',\n",
       " 'house',\n",
       " 'announce',\n",
       " 'monday',\n",
       " 'commentators',\n",
       " 'half',\n",
       " 'trillion',\n",
       " 'dollars',\n",
       " 'yukos',\n",
       " 'unit',\n",
       " 'buyer',\n",
       " 'faces',\n",
       " 'loan',\n",
       " 'claim',\n",
       " 'owners',\n",
       " 'embattled',\n",
       " 'russian',\n",
       " 'oil',\n",
       " 'are',\n",
       " 'ask',\n",
       " 'former',\n",
       " 'production',\n",
       " '900m',\n",
       " '£479m',\n",
       " 'state-owned',\n",
       " 'rosneft',\n",
       " 'bought',\n",
       " 'yugansk',\n",
       " '9.3bn',\n",
       " 'forced',\n",
       " 'russia',\n",
       " '27.5bn',\n",
       " 'tax',\n",
       " 'owner',\n",
       " 'menatep',\n",
       " 'group',\n",
       " 'says',\n",
       " 'repay',\n",
       " 'secured',\n",
       " 'similar',\n",
       " '540m',\n",
       " 'repayment',\n",
       " 'demand',\n",
       " 'banks',\n",
       " 'experts',\n",
       " 'would',\n",
       " 'include',\n",
       " 'obligations',\n",
       " 'pledged',\n",
       " 'so',\n",
       " 'real',\n",
       " 'money',\n",
       " 'creditors',\n",
       " 'avoid',\n",
       " 'seizure',\n",
       " 'moscow-based',\n",
       " 'lawyer',\n",
       " 'jamie',\n",
       " 'firestone',\n",
       " 'who',\n",
       " 'not',\n",
       " 'connected',\n",
       " 'case',\n",
       " 'managing',\n",
       " 'director',\n",
       " 'tim',\n",
       " 'osborne',\n",
       " 'told',\n",
       " 'reuters',\n",
       " 'news',\n",
       " 'agency',\n",
       " ':',\n",
       " 'if',\n",
       " 'they',\n",
       " 'default',\n",
       " 'we',\n",
       " 'fight',\n",
       " 'them',\n",
       " 'where',\n",
       " 'rule',\n",
       " 'law',\n",
       " 'exists',\n",
       " 'international',\n",
       " 'arbitration',\n",
       " 'clauses',\n",
       " 'credit',\n",
       " 'officials',\n",
       " 'unavailable',\n",
       " 'comment',\n",
       " 'take',\n",
       " 'action',\n",
       " 'recover',\n",
       " 'claims',\n",
       " 'debts',\n",
       " 'owed',\n",
       " 'filed',\n",
       " 'bankruptcy',\n",
       " 'protection',\n",
       " 'court',\n",
       " 'attempt',\n",
       " 'prevent',\n",
       " 'main',\n",
       " 'arm',\n",
       " 'went',\n",
       " 'sold',\n",
       " 'little-known',\n",
       " 'shell',\n",
       " 'turn',\n",
       " 'downfall',\n",
       " 'punishment',\n",
       " 'political',\n",
       " 'ambitions',\n",
       " 'founder',\n",
       " 'mikhail',\n",
       " 'khodorkovsky',\n",
       " 'vowed',\n",
       " 'sue',\n",
       " 'participant',\n",
       " 'high',\n",
       " 'fuel',\n",
       " 'ba',\n",
       " 'british',\n",
       " 'airways',\n",
       " 'blamed',\n",
       " '40',\n",
       " 'drop',\n",
       " 'reporting',\n",
       " '31',\n",
       " '2004',\n",
       " 'airline',\n",
       " 'pre-tax',\n",
       " '£75m',\n",
       " '141m',\n",
       " 'compared',\n",
       " '£125m',\n",
       " 'rod',\n",
       " 'eddington',\n",
       " 'respectable',\n",
       " 'costs',\n",
       " '£106m',\n",
       " '47.3',\n",
       " 'still',\n",
       " 'expectation',\n",
       " '£59m',\n",
       " 'rise',\n",
       " 'increased',\n",
       " 'price',\n",
       " 'aviation',\n",
       " 'last',\n",
       " 'introduced',\n",
       " 'surcharge',\n",
       " 'passengers',\n",
       " 'october',\n",
       " '£6',\n",
       " '£10',\n",
       " 'one-way',\n",
       " 'long-haul',\n",
       " 'flights',\n",
       " 'short-haul',\n",
       " 'raised',\n",
       " '£2.50',\n",
       " '£4',\n",
       " 'leg',\n",
       " 'yet',\n",
       " 'analyst',\n",
       " 'mike',\n",
       " 'powell',\n",
       " 'dresdner',\n",
       " 'kleinwort',\n",
       " 'wasserstein',\n",
       " 'estimated',\n",
       " 'annual',\n",
       " '£160m',\n",
       " 'short',\n",
       " 'additional',\n",
       " 'predicted',\n",
       " 'extra',\n",
       " '£250m',\n",
       " 'turnover',\n",
       " '4.3',\n",
       " '£1.97bn',\n",
       " 'further',\n",
       " 'benefiting',\n",
       " 'cargo',\n",
       " 'full',\n",
       " 'march',\n",
       " 'warned',\n",
       " 'yields',\n",
       " 'average',\n",
       " 'per',\n",
       " 'passenger',\n",
       " 'expected',\n",
       " 'decline',\n",
       " 'continues',\n",
       " 'face',\n",
       " 'competition',\n",
       " 'low-cost',\n",
       " 'carriers',\n",
       " 'forecast',\n",
       " 'total',\n",
       " 'outlook',\n",
       " 'previous',\n",
       " 'guidance',\n",
       " '3',\n",
       " '3.5',\n",
       " 'improvement',\n",
       " 'anticipated',\n",
       " 'martin',\n",
       " 'broughton',\n",
       " 'numbers',\n",
       " '8.1',\n",
       " 'january',\n",
       " 'nick',\n",
       " 'van',\n",
       " 'den',\n",
       " 'brul',\n",
       " 'bnp',\n",
       " 'paribas',\n",
       " 'described',\n",
       " 'latest',\n",
       " 'pretty',\n",
       " 'modest',\n",
       " 'quite',\n",
       " 'good',\n",
       " 'side',\n",
       " 'shows',\n",
       " 'impact',\n",
       " 'surcharges',\n",
       " 'positive',\n",
       " 'development',\n",
       " 'down',\n",
       " 'cost',\n",
       " 'very',\n",
       " 'since',\n",
       " '11',\n",
       " 'september',\n",
       " '2001',\n",
       " 'attacks',\n",
       " 'united',\n",
       " 'states',\n",
       " 'cut',\n",
       " '13,000',\n",
       " 'cost-cutting',\n",
       " 'drive',\n",
       " 'focus',\n",
       " 'reducing',\n",
       " 'controllable',\n",
       " 'debt',\n",
       " 'whilst',\n",
       " 'continuing',\n",
       " 'invest',\n",
       " 'products',\n",
       " 'example',\n",
       " 'delivery',\n",
       " 'six',\n",
       " 'airbus',\n",
       " 'a321',\n",
       " 'aircraft',\n",
       " 'month',\n",
       " 'start',\n",
       " 'improvements',\n",
       " 'club',\n",
       " 'world',\n",
       " 'flat',\n",
       " 'beds',\n",
       " 'shares',\n",
       " 'closed',\n",
       " 'four',\n",
       " 'pence',\n",
       " '274.5',\n",
       " 'pernod',\n",
       " 'takeover',\n",
       " 'talk',\n",
       " 'lifts',\n",
       " 'domecq',\n",
       " 'uk',\n",
       " 'drinks',\n",
       " 'food',\n",
       " 'allied',\n",
       " 'risen',\n",
       " 'speculation',\n",
       " 'target',\n",
       " 'france',\n",
       " 'ricard',\n",
       " 'reports',\n",
       " 'wall',\n",
       " 'street',\n",
       " 'journal',\n",
       " 'times',\n",
       " 'suggested',\n",
       " 'french',\n",
       " 'spirits',\n",
       " 'considering',\n",
       " 'bid',\n",
       " 'contact',\n",
       " '4',\n",
       " '1200',\n",
       " 'gmt',\n",
       " 'paris',\n",
       " 'slipped',\n",
       " '1.2',\n",
       " 'seeking',\n",
       " 'acquisitions',\n",
       " 'refused',\n",
       " 'specifics',\n",
       " 'seagram',\n",
       " 'propelled',\n",
       " 'global',\n",
       " 'top',\n",
       " 'other',\n",
       " 'two-thirds',\n",
       " 'leader',\n",
       " 'diageo',\n",
       " 'terms',\n",
       " '7.5bn',\n",
       " 'euros',\n",
       " '9.7bn',\n",
       " '9',\n",
       " 'smaller',\n",
       " 'capitalisation',\n",
       " '£5.7bn',\n",
       " '10.7bn',\n",
       " ';',\n",
       " '8.2bn',\n",
       " 'tried',\n",
       " 'buy',\n",
       " 'glenmorangie',\n",
       " 'scotland',\n",
       " 'premier',\n",
       " 'whisky',\n",
       " 'luxury',\n",
       " 'goods',\n",
       " 'lvmh',\n",
       " 'home',\n",
       " 'brands',\n",
       " 'including',\n",
       " 'chivas',\n",
       " 'regal',\n",
       " 'scotch',\n",
       " 'havana',\n",
       " 'rum',\n",
       " 'jacob',\n",
       " 'creek',\n",
       " 'wine',\n",
       " 'names',\n",
       " 'malibu',\n",
       " 'courvoisier',\n",
       " 'brandy',\n",
       " 'stolichnaya',\n",
       " 'vodka',\n",
       " 'ballantine',\n",
       " 'snack',\n",
       " 'chains',\n",
       " 'dunkin',\n",
       " 'donuts',\n",
       " 'baskin-robbins',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'wsj',\n",
       " 'two',\n",
       " 'consolidation',\n",
       " 'having',\n",
       " 'each',\n",
       " 'dealt',\n",
       " 'problematic',\n",
       " 'parts',\n",
       " 'their',\n",
       " 'portfolio',\n",
       " 'reduced',\n",
       " 'took',\n",
       " 'fund',\n",
       " 'just',\n",
       " '1.8bn',\n",
       " 'improved',\n",
       " 'fast-food',\n",
       " 'japan',\n",
       " 'narrowly',\n",
       " 'escapes',\n",
       " 'recession',\n",
       " 'economy',\n",
       " 'teetered',\n",
       " 'brink',\n",
       " 'technical',\n",
       " 'figures',\n",
       " 'show',\n",
       " 'revised',\n",
       " 'indicated',\n",
       " '0.1',\n",
       " 'similar-sized',\n",
       " 'contraction',\n",
       " 'basis',\n",
       " 'suggests',\n",
       " '0.2',\n",
       " 'suggesting',\n",
       " 'hesitant',\n",
       " 'recovery',\n",
       " 'common',\n",
       " 'definition',\n",
       " 'successive',\n",
       " 'negative',\n",
       " 'keen',\n",
       " 'play',\n",
       " 'worrying',\n",
       " 'implications',\n",
       " 'maintain',\n",
       " 'minor',\n",
       " 'adjustment',\n",
       " 'phase',\n",
       " 'upward',\n",
       " 'climb',\n",
       " 'monitor',\n",
       " 'developments',\n",
       " 'carefully',\n",
       " 'minister',\n",
       " 'heizo',\n",
       " 'takenaka',\n",
       " 'strengthening',\n",
       " 'yen',\n",
       " 'making',\n",
       " 'exports',\n",
       " 'indications',\n",
       " 'weakening',\n",
       " 'economic',\n",
       " 'observers',\n",
       " 'painting',\n",
       " 'picture',\n",
       " '...',\n",
       " 'patchier',\n",
       " 'paul',\n",
       " 'sheard',\n",
       " 'economist',\n",
       " 'lehman',\n",
       " 'brothers',\n",
       " 'tokyo',\n",
       " 'job',\n",
       " 'apparently',\n",
       " 'feed',\n",
       " 'through',\n",
       " 'domestic',\n",
       " 'private',\n",
       " 'consumption',\n",
       " 'slow',\n",
       " 'created',\n",
       " 'fewer',\n",
       " 'fall',\n",
       " 'jobseekers',\n",
       " 'pushed',\n",
       " 'unemployment',\n",
       " 'rate',\n",
       " 'lowest',\n",
       " 'years',\n",
       " 'according',\n",
       " 'labor',\n",
       " 'department',\n",
       " 'added',\n",
       " 'only',\n",
       " '146,000',\n",
       " 'gain',\n",
       " 'non-farm',\n",
       " 'payrolls',\n",
       " 'below',\n",
       " '190,000',\n",
       " 'nevertheless',\n",
       " 'push',\n",
       " '5.2',\n",
       " '2001.',\n",
       " 'mean',\n",
       " 'president',\n",
       " 'bush',\n",
       " 'celebrate',\n",
       " 'albeit',\n",
       " 'fine',\n",
       " 'margin',\n",
       " 'net',\n",
       " 'his',\n",
       " 'first',\n",
       " 'term',\n",
       " 'office',\n",
       " 'presided',\n",
       " 'over',\n",
       " 'november',\n",
       " 'presidential',\n",
       " 'election',\n",
       " 'herbert',\n",
       " 'hoover',\n",
       " 'creation',\n",
       " 'became',\n",
       " 'key',\n",
       " 'issue',\n",
       " 'adding',\n",
       " 'administration',\n",
       " 'record',\n",
       " 'ended',\n",
       " 'territory',\n",
       " '157,000',\n",
       " '133,000.',\n",
       " 'given',\n",
       " 'favourable',\n",
       " 'employment',\n",
       " 'expand',\n",
       " 'moderate',\n",
       " 'pace',\n",
       " 'rick',\n",
       " 'egelton',\n",
       " 'deputy',\n",
       " 'bmo',\n",
       " 'getting',\n",
       " 'got',\n",
       " 'low',\n",
       " 'relatively',\n",
       " 'environment',\n",
       " 'producing',\n",
       " 'satisfying',\n",
       " 'ken',\n",
       " 'mayland',\n",
       " 'clearview',\n",
       " 'economics',\n",
       " 'means',\n",
       " 'there',\n",
       " 'limited',\n",
       " 'number',\n",
       " 'opportunities',\n",
       " 'workers',\n",
       " 'india',\n",
       " 'fair',\n",
       " 'rules',\n",
       " 'attends',\n",
       " 'seven',\n",
       " 'leading',\n",
       " 'industrialised',\n",
       " 'nations',\n",
       " 'cowed',\n",
       " 'newcomer',\n",
       " 'status',\n",
       " 'lashed',\n",
       " 'restrictive',\n",
       " 'policies',\n",
       " 'objected',\n",
       " 'subsidies',\n",
       " 'agriculture',\n",
       " 'make',\n",
       " 'hard',\n",
       " 'developing',\n",
       " 'like',\n",
       " 'compete',\n",
       " 'called',\n",
       " 'reform',\n",
       " 'imf',\n",
       " 'palaniappan',\n",
       " 'chidambaram',\n",
       " 'argued',\n",
       " 'these',\n",
       " 'organisations',\n",
       " 'changing',\n",
       " 'order',\n",
       " 'integration',\n",
       " 'globalisation',\n",
       " 'engagement',\n",
       " 'attending',\n",
       " 'g20',\n",
       " 'thirds',\n",
       " 'population',\n",
       " 'conference',\n",
       " 'enterprise',\n",
       " 'hosted',\n",
       " 'gordon',\n",
       " 'brown',\n",
       " 'favour',\n",
       " 'floating',\n",
       " 'because',\n",
       " 'countries',\n",
       " 'cope',\n",
       " 'shocks',\n",
       " 'flexible',\n",
       " 'channel',\n",
       " 'absorbing',\n",
       " 'along',\n",
       " 'brazil',\n",
       " 'south',\n",
       " 'africa',\n",
       " 'invited',\n",
       " 'place',\n",
       " 'saturday',\n",
       " 'renewed',\n",
       " 'pressure',\n",
       " 'abandon',\n",
       " 'fixed',\n",
       " 'particular',\n",
       " 'surge',\n",
       " 'cheap',\n",
       " 'use',\n",
       " 'wish',\n",
       " 'judgements',\n",
       " 'separately',\n",
       " 'too',\n",
       " 'large',\n",
       " 'hamper',\n",
       " 'country',\n",
       " '6.5',\n",
       " '2005.',\n",
       " 'indian',\n",
       " '8.5',\n",
       " 'ethiopia',\n",
       " 'crop',\n",
       " '24',\n",
       " 'produced',\n",
       " '14.27',\n",
       " 'million',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word_smarter = pd.Series(word2idx.keys()).to_list()\n",
    "idx2word_smarter\n",
    "\n",
    "#ben buldum 4:44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f60c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of documents\n",
    "N = len(df[\"text\"])\n",
    "\n",
    "#number of words \n",
    "V = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efb1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate term-frequency matrix\n",
    "#note : same as using count vectorizer\n",
    "\n",
    "#altho more efficient to use sparse matrix, here in example we'll use dense matrix\n",
    "tf = np.zeros((N,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cdae47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x7f9e2195b980>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(tokenized_docs) #displays index and document at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba364dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate term-freq counts:\n",
    "#tokenized docs are not vectors they are just the documents translated into a list of index numbers for each of its word\n",
    "for i,doc_as_int in enumerate(tokenized_docs):\n",
    "    for j in doc_as_int: #j represents indices corresponded to words örneğin cümle (3,4,9) ise ilk kelime 3\n",
    "        #yani önce ilk dökümandaki 3 nolu kelimeyi gösteren hücre 1 artacak, tekrar 3 kelimesi aynı doc'ta çıksa\n",
    "        #gene artacak, böyle böyle i satırında (doc'unda) her kelimenin frequency'sini görcez\n",
    "        tf[i,j] += 1\n",
    "        #sonra her döküman için yapcaz bunu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3534126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute IDF : her kelimenin toplam kaç dökümanda göründüğüne bakcaz\n",
    "\n",
    "#remember we'll have idf value for each word, so v idf values, a vector of size v\n",
    "document_freq = np.sum(tf >0, axis = 0) #this is a v size array \n",
    "\n",
    "#tf>0 a boolean operator, her kelime(sütun) için tf = 0 olmayan dökümanlarsa true diyecek, toplamları döküman sayısı\n",
    "#çok mantıklı, tek sıkıntı sum diyince aşağı doğru topladığına nerden eminiz? axis = 0 dediğimiz mi için?\n",
    "idf = np.log(N / document_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69ca7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute TF-idf\n",
    "tf_idf = tf * idf\n",
    "\n",
    "##hocada bu warningler çıkmıyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2855628d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: sport\n",
      "Text: Athens memories soar above lows\n",
      "Top 5 terms:\n",
      "paula\n",
      "athens\n",
      "1500m\n",
      "her\n",
      "kelly\n"
     ]
    }
   ],
   "source": [
    "#pick a random document, show first 5 terms in terms of tf_idf score\n",
    "np.random.seed(123)\n",
    "i = np.random.choice(N)\n",
    "row = df.iloc[i]\n",
    "\n",
    "print(\"Label:\", row['labels'])\n",
    "print(\"Text:\", row['text'].split(\"\\n\", 1)[0]) ##we're printing out the first line of text, not the entire text\n",
    "#recall each news article has multiple lines\n",
    "#for simplicity we'll split the test with the new line character : \"\\n\"\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = tf_idf[i] #tf_idf matriksinde i dokümanına ait satırdaki her kelimeye ait tf-idf value'leri\n",
    "indices = (- scores).argsort() ##en yüksek if-tdf'e sahip kelimeden azalarak o value'lere sahip kelimelrein indeksleri\n",
    "\n",
    "for j in indices[:5]: #en yüksek iftdf'li 5 kelimenin endeksleri\n",
    "    print(idx2word[j]) #o endeks key ise value pair'i nedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb5ee466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paula\n",
      "athens\n",
      "1500m\n",
      "her\n",
      "kelly\n"
     ]
    }
   ],
   "source": [
    "##bakalım benim idxtoword'ümle olcak mı\n",
    "for j in indices[:5]: \n",
    "    print(idx2word_smarter[j]) \n",
    "    \n",
    "##YESSS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5484b",
   "metadata": {},
   "source": [
    "## Exercise: use CountVectorizer to form the counts instead\n",
    "\n",
    "## Exercise (hard): use Scipy's csr_matrix instead\n",
    "## You cannot use X[i, j] += 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4068cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7edb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b769e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc6e20e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[235, 61, 69, 236, 237, 23, 235, 80, 238, 56, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[400, 401, 402, 403, 404, 405, 23, 406, 37, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[510, 511, 327, 238, 512, 92, 7, 513, 514, 80,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[662, 663, 664, 665, 666, 657, 40, 667, 668, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "1  [235, 61, 69, 236, 237, 23, 235, 80, 238, 56, ...\n",
       "2  [400, 401, 402, 403, 404, 405, 23, 406, 37, 40...\n",
       "3  [510, 511, 327, 238, 512, 92, 7, 513, 514, 80,...\n",
       "4  [662, 663, 664, 665, 666, 657, 40, 667, 668, 4..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = pd.Series(tokenized_docs)\n",
    "tokenized_docs = pd.DataFrame(tokenized_docs)\n",
    "tokenized_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ba5996d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'as_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/9bnp6m5d7db0pky3235l4wgh0000gn/T/ipykernel_1599/134913248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/5d/9bnp6m5d7db0pky3235l4wgh0000gn/T/ipykernel_1599/134913248.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'as_type'"
     ]
    }
   ],
   "source": [
    "tokenized_docs[0].apply(lambda x: x.as_type(\"str\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdd8e062",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/9bnp6m5d7db0pky3235l4wgh0000gn/T/ipykernel_1599/4044668410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tf_cv = vectorizer.fit_transform(tokenized_docs)\n",
    "tf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##YAPAMADIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
