{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e212ce3",
   "metadata": {},
   "source": [
    "# FeedForward Artificial Neural Networks\n",
    "- feedforward NN are a specific kind of NNs\n",
    "- this is the most basic kind, other forms are CNN ,RNN\n",
    "- inspired by human brain but a very simplistic way: left side are inputs and right side are outputs\n",
    "- in reality, wires can criss-cross in human brain, a later neuron can connect back to an earlier neuron to create a recurrent connection\n",
    "- NNs we're about to discuss in this section contains no such complexities\n",
    "- since the input is on the one side and output on the other, we can go from input to output from left to right, why we called feedforward\n",
    "\n",
    "#### isntead of Tf-idf we'll learn to use word embeddings, but we won't actually use them until CNN or RNN, they only come into play then. yet as an exercise: you can use word2vec to pre-train word embeddings\n",
    "- word2vec is acutally a NN which we'll learn in this course\n",
    "- so you can apply word2vec, it's advanced bcz code is more complex\n",
    "\n",
    "### Forward Propogation\n",
    "- remember,a model is about making predictions\n",
    "- let's say we have three inputs that goes to one neuron (original model)\n",
    "- now let's think these inputs propogate also to another neuron\n",
    "- each of these neurons can be calculating something differnet (via different weights)\n",
    "- ex: we're looking at our face, one neuron might be looking for the presene of an eye, another for the presence of a nose\n",
    "- In other words different neurons are finding different features from the input\n",
    "- if we add one more: we'll get a full layer of neurons (3e 3 olduğu için mi?)\n",
    "#### What we do if we have multiple neurons per layer?\n",
    "- say there are M neurons\n",
    "- call the output of jth neuron Zj\n",
    "- so j = 1, ...M \n",
    "- we can make this calculation more efficient if we think z as a vector of values instead of just a scalar\n",
    "- z becomes a vector of size M (column vector size Mx1)\n",
    "- x is a vector of size D (D sayıda weight var) (column vector size Dx1)\n",
    "- W is a matrix sized DxM olmalı\n",
    "- b is a vector of size M\n",
    "#### Input to Output for L-layer NN\n",
    "- we need to calculate a matrix of W and a vector of b for each L layers.\n",
    "- in the end p(y=1|x) = ö * (W^L.T * z^(L-1) + b ^L ) if we're doing a binary classification\n",
    "- we wouldn't have the sigmoid there if we were doing regression\n",
    "- for regression just remove the final sigmoid.\n",
    "- diğer aşamalardakiler duruyo\n",
    "#### Another perspective: each NN layer is a feature transformation\n",
    "- we're just doing a series of feature transformations\n",
    "- researchers noticed each layer learns something more complex than the last\n",
    "- this is the motivation for building NNs that are more deep\n",
    "- that's why it's called DL \n",
    "- Feature Hierarchies!\n",
    "\n",
    "### Geometrical Picture\n",
    "- he always says ML is nothing but a geometry problem\n",
    "- One neuro, linear model, can get us only at some point\n",
    "- we can only differentiate linear boundaries\n",
    "- 2 ways we can make our problem more complicated than \"finding the line\":\n",
    "    - add more input dimensions\n",
    "    - make the pattern nonlinear #this is what we interested in when we discuss NNs\n",
    "- W.T*x + b will always give us hyperplane, there is no other option, but let's say we wanna get. circle as boundary\n",
    "- how?\n",
    "- One method is to use:\n",
    "#### Feature Engineering\n",
    "- ex: suppose salary is a quadratic function - it's still a linear model\n",
    "- Problem: too many possibilites, too many different features\n",
    "- also consider the interaction terms\n",
    "- if 2 inputs: x1, x1^2, x2, x2^2, x1* x2\n",
    "- what happens if I have tooooo many inputs?\n",
    "- number of terms we have to consider grows very fast and thus feature engineering even in this form seems to be a clumsy solution\n",
    "#### Let's go back to neurons\n",
    "- each neuron compute a different nonlinear feature derived from all of the inputs\n",
    "- nonlinear bcz sigmoid\n",
    "- you can think those as feature 1, feature 2, feature 3, all the way to feature M.\n",
    "- two-layer NN denkleminde z'yi yerine yaz, ipad'de: you cannot reduce the NN equation into the linear form bcz of the presence of sigmoid\n",
    "#### Automatic Feature Engineering\n",
    "- an added bonus of using neurons for our features\n",
    "- remember Ws and bs are randomly initialized and found iteratively using gradient descent\n",
    "- this process automatically does feature engineering\n",
    "- In the older days of ML, feature engineering was the only way to apply ML successfully and this requires domain knowledge!\n",
    "- DL allows people who are not domain experts to build models, only expertise in ML itself is enough\n",
    "#### Tensorflow playground\n",
    "- lets you train a NN on syntethic data on browser\n",
    "- you can practice and see a NN learning a nonlinear decision boundary\n",
    "- donut problem\n",
    "- baya eğlenceli görünüyo, try diffrent models with different numbers of hidden layers and different numbers of units per hidden layer\n",
    "\n",
    "### Activation Function\n",
    "- we've so far looked at sigmoid: maps values to zero(0...1) mimics the biological neuron\n",
    "- it makes decision boundary non-linear\n",
    "- unfortunately there are some problems with sigmoid, so it's no longer used as often, except in some specific cases\n",
    "#### Standardization\n",
    "- remember we don't want our inputs to have too different ranges\n",
    "- rather we want them to center around zero and have a similar range\n",
    "- sigmoid is problematic in this regard, output is between 0 and 1, middle value is 0.5, output can never be centered around zero\n",
    "- Recall the concept of uniformity of neural network ## unuttum nerde geçiyodu\n",
    "- one neuron takes as input the output of the previous layer, so previous layer's output centering around 0.5 is not quite right\n",
    "- therefore we want both the inputs and the outputs to be standardized\n",
    "#### Solution: Hyperbolic Tangent function (tanh)\n",
    "- instead of using sigmoid we'll use a function with exact shape of sigmoid, just centered around zero\n",
    "- yet the story not ober, with tanh though better than sigmoid still problems !\n",
    "- The major problem with sigmoid and tanh is the Vanishing Gradient Problem\n",
    "#### Vanishing Gradient Problem\n",
    "- remember we used graident descent to train NN.\n",
    "- the problem is when you have a very deep model\n",
    "- the deeper neural network is, the more terms will have to be multiplied in the gradient due to the chain rule of calculus\n",
    "- bcz NN output is basically just a bunch of composite functions nested: sigmoid of sigmoid of sigmoid...\n",
    "- so composite functions become multiplication in derivative\n",
    "- in other words we end up by multiplying with the derivative of sigmoid over and over again\n",
    "- most of the sigmoid is already very flat, so it's derivative is zero in most places, only changes near zero,\n",
    "- highest possible value 0.25\n",
    "- imagine keeping multiplying them\n",
    "- the further you go back in the backpropogation, the smaller the gradient gets.\n",
    "- remember the training algorithm takes small steps in the direction of gradient\n",
    "- if gradient is nearly zero, that means updates to the weights will also be nearly zero\n",
    "- ending result is that weights close to the input of NN, will almost be not trained at all.\n",
    "#### Old School Solution:\n",
    "- greedy layer-wise pre-training invented by Geoffry Hinton\n",
    "- train each layer greedily, oone after another\n",
    "- train each layer using some alternative loss function\n",
    "- once you train the first layer, you add a second layer and train that by itself without touching the first layer anymore, then add a third layer.. when you get to the last layer, all previous layer would already be trained to some extent\n",
    "#### Current solution : ReLu\n",
    "- just don't use activation functions that have vanished the gradient :D\n",
    "- Rectifier Linear Unit\n",
    "- the values greater than zero will never have a zero gradient\n",
    "- Relu doesn2t have a vanishing gradient bcz the gradients on the left side of the graph has already vanished\n",
    "- This phenomenon is called \"dead neuron problem\"\n",
    "#### Dead Neuron Problem\n",
    "- the neurons that always output to zero because the weighted sum of its inputs are always <= 0.\n",
    "- but in DL, what we care about is experimental results\n",
    "- In other words, ReLu works\n",
    "- the fact that right side not vanished seem to be enough\n",
    "- yet some researchers tried to fix dead neuron problem\n",
    "- One alternative is \"Leaky ReLu\": small positive slope for negative inputs (like 0.1)\n",
    "- still a nonlinear function that can learn nonlinear patterns\n",
    "- slope always positive\n",
    "- There are other options as well like ELU, exponential linear unit\n",
    "- authors argue that speeds up learning and get higher accuracy\n",
    "- it also allows negative values, so the mean can be zero, it can be centered around zero.\n",
    "#### e abi hakkaten biz ReLu kullanınca standardization ı kaybetmedik mi?\n",
    "- another option: Softplus\n",
    "- note: both softplus and Elu has a vanishing gradient on the left\n",
    "- but we already know it's not very problematic, otherwise ReLu wouldn't have worked\n",
    "#### although we initially stated that you would want the output to each layer centered around zero, you can see ReLu and Softplus don't accomplish this, they range from zero to infinity\n",
    "- these days most people still use ReLU as a reasonable default\n",
    "- try ELU or Softplus, LReLU and see, sometimes they do better\n",
    "- actually ReLu is more biologically plausible than sigmoid\n",
    "\n",
    "### Extra reading: BRU activation functions may perform better than ELU, ReLU in case you wanna apply on your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde031da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
