{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb6133d",
   "metadata": {},
   "source": [
    "# Class Imbalances and Methods to deal with them\n",
    "- ex: disease - no disease\n",
    "- diseases are rare in population 0.1 % have it and 99% don't\n",
    "- suppose our classifier is dumb and predicst no disaese, it will be right 99% of time\n",
    "- hence accuracy not always a good metrics\n",
    "\n",
    "- let's assume testing for disease is indiscriminate, like everyone in population has equal chance for getting tested\n",
    "\n",
    "### What can we do?\n",
    "- we can start by looking at ore detailed metrics, false positives and negatives\n",
    "- sütunlar actual class; satırlar predicted class\n",
    "- Confusion matrix is general and can be drawn for any number of classes\n",
    "- but for this lecture we'll look for binary classification\n",
    "\n",
    "### Sensitivity and Specificty\n",
    "- the metrics you use depend on the field you come from\n",
    "- mecdical / life sciences tend to semsitivtiy and specificty\n",
    "- Sensitivity is the True Positive Rate = TP / TP + FN (share of people we diagnosed with disease among all people with disease), how good we're in detecting disease in ppl who actually has it.\n",
    "- Specificity is the true negative rate = TN / FP + TN (percentage of negatives you able to detect correctly)\n",
    "\n",
    "#### suppose what happens to these two metrics if we use a very bad classifier that always predicts no disease\n",
    "- let's say 0.001of N people have disease 0.999 not\n",
    "- TN : 0.999N everyone w/o disease and we corectly predict\n",
    "- FN : 0.001N we couldn't predict tho they hsve disease\n",
    "- TP : 0, FP:0\n",
    "- sensitivity = 0, bad sign\n",
    "- specificity = 1 good sign\n",
    "\n",
    "#### what if our model makes perfect preds?\n",
    "- FN = 0 FP = 0\n",
    "- Sensitivity = 1\n",
    "- Specificty = 1\n",
    "- in the ideal classifier\n",
    "\n",
    "### Precision and Recall\n",
    "- used in the field of NLP \n",
    "- Recalls = Sensitivity = True Positive Rate, ex in terms of doc search engines doc youcorrectly retrieved / doc you should have found ( % documents yo're able to recall)\n",
    "- Precision: Positive Predicted Value TP / TP + FP (documents correctly retrived / all documents retrieved)\n",
    "- if you have a retrieval system that retrieves all the document every time precision will be very low, imprecise\n",
    "- if no irrelevant documents = very precise, precision =1\n",
    "\n",
    "#### consider you comparing two models, one with better precision the other is better recall, how to choose the best? \n",
    "- distill this metrics back into a single number:\n",
    "### F1-score: \n",
    "the harmonic mean of precision and recall\n",
    "F1 = 2 * precision* recall / (precision + recall)\n",
    "\n",
    "### ROC curve: One more metric to discuss\n",
    "- era of radar / estimation & detection theory\n",
    "- \"Receiver Operating Characteristics)\n",
    "- True Positive Rate on y axis, and False Positive Rate on x-axis\n",
    "- Note: FPR = 1- TNR\n",
    "- Main idea behind: Decision rule can be based on a threshold\n",
    "- there is a tradeoff btw specificty and sensitivty\n",
    "#### let'say we have an enemy ship detector with low sensitity (failing to detect eenemy ships)\n",
    "- one solution : we can lower the detection threshold\n",
    "- normally we choose to detect if probability is greater than 50 % , we can lower it to 40 or 30%\n",
    "- this will yield more positive detecitons hopefully increasing TPR\n",
    "- yet this can increase also FP so decrease the specifity\n",
    "- grafiğin sol üst köşesi en iyi situation'ı gösteriyo\n",
    "- if we mess up we should look at this graph, we can get a single number from this graph: AUC\n",
    "\n",
    "#### AUC= Area Under ROC curve\n",
    "- Perfect AUC = 1 (the whole box is under the curve)\n",
    "- benchmark AUC (random guessing) = 0.5\n",
    "- so this is another alternative to F1 score\n",
    "\n",
    "#### ROC requires that our model output probabilities\n",
    "- In Sklearn model.predict() returns a 1D array with N-lenght\n",
    "- if there are K classes array will contain integers from 0 to K-1\n",
    "- model.predict_proba() returns the probabilities\n",
    "- will this be the API forever? who knows it's a weird name\n",
    "- suppose we have N samples and K classes, this method returns a matrix of NxK\n",
    "- outputs are not integers but floating points\n",
    "- P(n,k) posterior probability prob(y=k | xn) if the output variable is called P\n",
    "- naturally each row of this array will sum to 1. bcz row represents a prob distribution\n",
    "- so if we have to classes P[:, 1] shows the probability of happening for each sample\n",
    "\n",
    "#### AUC in sklearn\n",
    "roc_auc_score(Y, P[:,1])\n",
    "\n",
    "#### NOTE: not all classifiers in sklearn has .predict_proba( ) function\n",
    "- simply due to how certain classifiers work, not all of them output probabilities\n",
    "- e.g. perceptron\n",
    "- SVM doesn't but implementation contains ad hoc method to obtain proabbilities yet it has some issues:\n",
    "- probabilities may not be consistent with predictions\n",
    "- doesn't scale for large datasets\n",
    "\n",
    "#### on the other hand, since neural networks output probs, AUC in deep learning is fine choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3b6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
