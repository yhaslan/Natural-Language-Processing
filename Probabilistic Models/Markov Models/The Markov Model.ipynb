{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5357431e",
   "metadata": {},
   "source": [
    "# The Markov Model\n",
    "\n",
    "for this section we'll consider sequences of categorical symbols which we'll refer as states:  sunny, rainy, cloudy\n",
    "\n",
    "başka bi section da part-of-speech tags olabilir noun,verb, etc\n",
    "\n",
    "s(t) = s_t = state at time t\n",
    "\n",
    "we'll number the states from 1 to M, with M showing total number of possible states, M = 3 for sunny / rainy / cloudy\n",
    "\n",
    "p(s_t = i) means probability of state being i at time t\n",
    "\n",
    "\n",
    "### State Distribution\n",
    "\n",
    "we can have such a probability for all values of i\n",
    "\n",
    "p(s_t = 1), p(s_t = 2),..., p(s_t = M) #M probabilites - together they form a probability distribution\n",
    "\n",
    "it has a special name: state distribution\n",
    "\n",
    "p(s_t) refers to the state distrbiution (M vectors length?)\n",
    "\n",
    "p(s_t = j | s_t-1 = i) t-1'de state y ise t'de state'in j olma ihtimali\n",
    "\n",
    "#### note this is a conditional distribution, question : how many of these probability values for i and j?\n",
    "\n",
    "since both can take M values, M^2\n",
    "\n",
    "### State Transition Matrix\n",
    "\n",
    "notice it's quite tedious to write p(s_t = j | s_t-1 = y)\n",
    "\n",
    "Instead we simply store those values in a matrix. Convention:\n",
    "- first index: previous state, second index: next state\n",
    "- MxM matrix\n",
    "\n",
    "#### Biraz ipad'e yazdım bunu\n",
    "\n",
    "### Initial State Distribution\n",
    "\n",
    "State Transition dist sadece bi state'ten diğereine geçme olasılıklarını gösterir\n",
    "\n",
    "Bi de ilk state için prob distributionları görmemiz gerek\n",
    "\n",
    "#### so we need to find A (state transition distribution) and π (initial state distribution) , how?\n",
    "\n",
    "### Probability of a sequence:\n",
    "\n",
    "Normally p(s1...t) = p(s1) * p(s2 | s1)* p(s3 | s1,s2) *...* p(st | s1, s2,..., st-1)\n",
    "\n",
    "Thanks to Markov assumption: = p(s1)* p(s2 | s1) * ... p(st|st-1) = p(s1) * /// 1'den t ye p(st | st-1) geometrik dizi\n",
    "\n",
    "ipad'e bak.\n",
    "\n",
    "### Training a Markov model: Estimating A and π\n",
    "\n",
    "^π = counts (s1 = i) / N yani ilk kelimenin i olduğu durumların sekans sayısına oranı. N datasetteki sekans sayısı\n",
    "Âij = count(i --> j) / count (i) yani i'den sonra j gelen case'lerin toplam i caselerine oranı\n",
    "\n",
    "## Probability Smoothing and Log\n",
    "\n",
    "#### yalnız buna adjustment gerek çünkü mesela Test - Train yaparken diyelim bi state transition hiç gerçekleşmemiş training sample'da p(st | st-1) 'ler birbiriyle çarpıldığı için o kelime sekansı tüm probability yi sıfır yapıyo. yani test sample da karşımıza o sekansa sahip bi cümle çıksa o cümleye biz imkansız diyoruz. Çözüm very low da olsa her sekansa probability ssigtn etmek.\n",
    "\n",
    "### +1 ekleme yöntemi: (Add One Smoothing)\n",
    "\n",
    "- add a fake count 1 to each Aij transition: [count(i-->j) + 1] / [count(i) + M ] \n",
    "- adding also M to denominator ensures each rows of A sums to 1. (Nasıl?)\n",
    "- we can do this also to the initial state distribution ^π = [counts(s1 = i) + 1] / N + M\n",
    "\n",
    "### Add epsilon smoothing (a simple extension to add one smoothing):\n",
    "In case we think adding one is a bit too biased\n",
    "Again this will ensure each row in Aij summing to 1.\n",
    "\n",
    "- make more smoother (e>1) or less smooth(e<1) \n",
    "- Aij =  [count(i-->j) + e] / [count(i) + eM ] \n",
    "- ^π = [counts(s1 = i) + e] / N + eM\n",
    "#### what is more smooth less smooth\n",
    "\n",
    "### Computing the probability of a sequence\n",
    "- remember this involves multiplying the probability of each word, i.e multiplying many small numbers as there are many words in eng language\n",
    "- so you get closer and closer to zero, computer doesn't have infinite precision so end up rounding them down to zero.\n",
    "- very problematic if for ex we wanna compare to sentences\n",
    "\n",
    "### Solution: log probabilities\n",
    "- It's ok since we dont wanna values but the comparison. \n",
    "formula: log p(s1...T) = log p(πs1) + SUM log p(Ast-1,st) t=2...T aritmetik dizisi\n",
    "- another benefit: in computer sum is faster than multiply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c3029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
