{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a392a15d",
   "metadata": {},
   "source": [
    "# RNN \n",
    "- so far we made an assumption for our NNs different from biological neurons\n",
    "- that is all the information goes from left to right\n",
    "- why not have neuronal connections which are less restrictive? connections that can go backwards\n",
    "- When we create connections that can go backwards in a specific way with a time delay of one step, we call them recurrent neural networks\n",
    "\n",
    "#### note: there are all kinds and ways to make backwards connections but there is only one specific way that is called recurrent neural networks. Diagram: \n",
    "- 1 hidden layer in which every hidden unit has an arrow that goes back towards every other hidden unit. INCLUDING ITSELF\n",
    "\n",
    "- we'll start with simple RNN also aka Elman RNN\n",
    "- Code exercise on shapes to reinforce our understanging\n",
    "- well, simple RNN almost never used anymore, for the time being LSTM is the most popular unit\n",
    "- LSTM and GRU - what advantage they have over simple RNN\n",
    "- practice on datasets\n",
    "\n",
    "#### Recall: W.T * x + b is just a neuron!\n",
    "\n",
    "## Simple RNN / Elman Unit\n",
    "- imagine you use ANN for classifying words, e.g. Part of speech tags, or spam detection\n",
    "- there are multiple diferent answers depending on the context\n",
    "- the context the word bank used, should change the chance of being classified as spam or not\n",
    "- hidden features (features transformed in hidden layers) also called hidden representations\n",
    "- hidden layers are the network, output to these hidden layers are hidden representations of the input\n",
    "- back to our classification problem:\n",
    "- for each NN, instead of taking in a single input at the current timestamp, it also takes the hidden representation from the previous timestamp\n",
    "- notice: by doing this, there is a pathway from all previous words to current output!\n",
    "- sanırım bu noktada markov model'lerden ayrılıyo, çünkü sadece bi öncekine bağlı değil.\n",
    "- it does not just depend on the previous timestamp but all the past timestamps through the previous hidden representations, so this is called the recurrent NN.\n",
    "- hidden representations = hidden states. A common terminology dealing with the sequences\n",
    "- typically we assume the first hidden state is a vector of zero, seeipad for equation\n",
    "- as usual it's useful to think about shapes: can we infer what the shapes of our weights and bias vectors should be?\n",
    "- M : size of hidden vector. D: size of input vector.\n",
    "- Wh. : MxM\n",
    "- Wx : DxM\n",
    "- bh and bx : M\n",
    "\n",
    "### How to use Elman Unit to solve problems?\n",
    "- 1) Many to one tasks (ex. Spam Detection or Sent Analysis): you have a whole seq of inputs but only single output\n",
    "- 2) Many-to-Many tasks (ex. Parts of Speech tagging or Time series anomaly detection): you have a seq of inputs and a seq of outputs.\n",
    "For the anomaly detection, any point in a time series could be classified as anomalous\n",
    "\n",
    "- our network will end with a final dense layer as it always does and a final activation which is appropriate for the task in hand\n",
    "- yet thereare multiple ways to connect the final dense layer\n",
    "- h(t) is the output of RNN in every time step, the question is what to do with all these hidden outputs?\n",
    "- answer depends on the type of task\n",
    "- if we're doing many-to-one, we only keep the final hidden state, which contain all the info from the time series, pass only that to the final dense layer, so take only the final hidden state or keep global max pool over past hidden states, and keep max\n",
    "- if we're doing a many-to-many task, then keep all the hidden states, each of which contain the info only up to that point, pass all those hidden states to the final dense layer to get a big tree of separate predictions, one for ech timestamp.\n",
    "- NOTE: same dense layer is applied to all timestamps, just as the same weights are used at every timestamp (another example of weight-sharing) = same simple RNN applied in all timestamps\n",
    "\n",
    "#### Consider a CNN with conv > pool > conv > pool... with TxD input and TxM output\n",
    "- well the RNN gives us the exacly same shape bcz we'll be using T hidden vectors which are of size M.\n",
    "- they are not different things but they're different perspectives on the same kind of data. (convolutions vs RNNs).\n",
    "\n",
    "#### Just as in CNN before passing our data to final dense layer, we need to obtain a single flat feature vector.\n",
    "- In both cases we reduce TxM to M.\n",
    "- one way to obtain such a feature vector is global max pooling\n",
    "- In RNN, it takes the max value over time, such that you end up with M different features. Put simply, we're getting rid of the time dimension!\n",
    "- It makes sense to take maximum since we use that as a proxy to which value matters most\n",
    "- ex: think of a movie review, suppose word \"terrible\" appears but not towards the end of sentence. Due to the vanishing gradient problem, RNN may not be able to recognize the word terrible if it appears too far away from the end. However by taking max, we can look at all the hidden values from each previous timestamp, which let us see more clearly words mattering most for predicting the target.\n",
    "\n",
    "### ANN vs RNN\n",
    "- for many-to-many case, let's think what shape of output will be.\n",
    "- ANN: D input, M hidden units, K output\n",
    "- RNN : T x D shaped input sequence, T x M shaped hidden vector sequence (that means every time stamp gets its own hidden state vector), after passing each of these hidden state vector from the final dense layer, each of output will have shape K, so we'll have a sequence of shape TxK\n",
    "- so imagine our task is to predict parts of speech tags (there are 8 of them), then K would be 8.\n",
    "- you can see how this is analogous to ANN\n",
    "\n",
    "### Can you stack multiple RNN layers\n",
    "- Yes\n",
    "- if the output of a hidden layer is TxM1, this is just another time-series, so we can pass this to another hidden layer and get another hidden output shaped TxM2, which is yet another timeseries\n",
    "#### Note: one common mistake is to confuse T and M: The sequencec length is not the same as the size of hidden vector\n",
    "- normally we wouldn't change the size of the hidden layers (M1 = M2)\n",
    "- previously when you see a circle in a diagram for ANN, you can think of it as a single number\n",
    "- yet, for RNN diagrams, notice each circle does not represent a number but a whole RNN in a layer which outputs a vector\n",
    "\n",
    "#### we studied Elman Unit, but these ideas on RNN units can apply also on LSTM and GRU, interface is same in all cases\n",
    "- the unit themselves are different but the way they incorporates to RNN are same\n",
    "\n",
    "## Code Preparation\n",
    "model structure stays the same"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6162d5c9",
   "metadata": {},
   "source": [
    "model = Model(inputs, outputs)\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "r = model.fit(Xtrain,\n",
    "              Ytain,\n",
    "              epochs = 50, \n",
    "              validation_data = (Xtest, Ytest)\n",
    "              predictions = model.predict(Xtest))\n",
    "\n",
    "# Xtain and Xtest are now NxTxD like CNN, T: sequence length,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d8cc7",
   "metadata": {},
   "source": [
    "#### In Tensorflow: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "44422299",
   "metadata": {},
   "source": [
    "i = Input(shape = (T, D))\n",
    "x = SimpleRNN(M)(i) ## there is a self loop in the hidden layer\n",
    "x = Dense(K)(x)\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818617b3",
   "metadata": {},
   "source": [
    "there is a self loop in hidden RNN, self loop means simple RNN and no loop means Dense\n",
    "- for RNNs the default activation function is Hyperbolic tangent tho you can use other activation functions if you like, this is unlike the dense layer where the default activation is Identity\n",
    "- let's again think by the shape of data in Many-to-one\n",
    "    - we start with a multivariate timeseries shaped TxD\n",
    "    - this then goes thru a simpleRNN layer\n",
    "    - SimpleRNN takes in a timeseries of vectors, x1, x2 all the way to XT (each vector sized D)\n",
    "    - it converts them into a timeseries of hidden vectors, h1, h2... hT (each vector sized M (genelde))\n",
    "    - each hidden vector depends on the current input X + past hidden vector\n",
    "    - we only keep hT (sized M)\n",
    "    - final dense layer like a regular ANN > output yT (a vector of sized K)\n",
    "    \n",
    "#### now let's consider shape in many-to-many case:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "984664c9",
   "metadata": {},
   "source": [
    "i = Input(shape=(T,D))\n",
    "x = SimpleRNN(M, return_sequences = True)(i)\n",
    "x = Dense(K)(x)\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4c170",
   "metadata": {},
   "source": [
    "this will return all the hidden vectors in a timeseries, we get all the hidden vectors in a single array, of size TxM\n",
    "- suppose this is a many-to-many task where we want a prediction for each timestamp\n",
    "- produced y1, y2, ... yT : each of y is sized K. TxK\n",
    "- then we can pass the output automatically thru a Dense Layer and the output will be TxK\n",
    "- you don't need any extra argument for Dense Layer. Tensorflow will automatically know whether handling a single vector vs handling a time-series of vectors.\n",
    "- if you pass single vector, you'll get back a vector of size K.\n",
    "- if you pass a series of vectors, you will get back a series of vectors, each of which of size K.\n",
    "\n",
    "#### another scenario: we're dealing again with many-to-one task but we need global max pooling:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d3b0456",
   "metadata": {},
   "source": [
    "i = Input(shape=(T,D))\n",
    "x = SimpleRNN(M, return_sequences = True)(i)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(K)(x)\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca364e0c",
   "metadata": {},
   "source": [
    "that's when you wanna keep all hidden values and take the maximum value over time, apply globalmaxpooling just as we did wirh CNNs\n",
    "- this compare a timeseries of TxM into a single vector of size M, completely eliminating the time-dimension, then we move to our typical final dense layer\n",
    "\n",
    "#### last scenario : stacking Layers\n",
    "- when you stack multiple RNN layers together\n",
    "- recall :  the input of RNN must be a timeseries, therefore the output of the previous layer must also be a timeseries\n",
    "- we know that this can be done by setting the return_sequences = True"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fce676b",
   "metadata": {},
   "source": [
    "i = Input(shape=(T,D))\n",
    "x = SimpleRNN(32, return_sequences = True)(i) #RNN layer with 32 hidden units\n",
    "#the output is still a time-series so it can further be passed to other RNN layers\n",
    "x = SimpleRNN(32)(x) #let's make the next RNN layer with also 32 hidden units, but return_sequences is false (default)\n",
    "#so output of this layer will just be a vector of size 32.\n",
    "x = Dense(K)(x)\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2f840",
   "metadata": {},
   "source": [
    "### Easy to use LSTM and GRU (Preview)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49fc9c6e",
   "metadata": {},
   "source": [
    "i = Input(shape=(T,D))\n",
    "x = LSTM(M)(i)\n",
    "x = Dense(K)(x)\n",
    "model = Model(i,x)\n",
    "\n",
    "i = Input(shape=(T,D))\n",
    "x = GRU(M)(i)\n",
    "x = Dense(K)(x)\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c325ade",
   "metadata": {},
   "source": [
    "## RNN: Paying Attention to Shapes \n",
    "- yine bende olmayan bi colab notebook açtı te allam :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987dad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0a3bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 17:31:28.035828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6ecbc",
   "metadata": {},
   "source": [
    "#### Things you should automatically know and have memorized:\n",
    "- N = number of samples\n",
    "- T = sequence length\n",
    "- D = number of input features\n",
    "- M = number of hidden units\n",
    "- K = number of output units\n",
    "\n",
    "#### sidenote: K > 1 not automatically means you doing a classification with softmax, you can do a multidimensional regression as well\n",
    "- imagine you're trying to predict lat-long coordinates, K would be two but still a reg problem\n",
    "\n",
    "#### ya bu multidim regression olayı benim çok aklıma yatmadı, iki output'un illa birbiriyle alakası oluyo mu ki o durumda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7340164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some data:\n",
    "N = 1 #sample\n",
    "T = 10 #sequence length\n",
    "D = 3 # feature number (vector dimensionality)\n",
    "K = 2\n",
    "X = np.random.randn(N, T, D) #bu ne demek ya, our X will be shaped NxTxD tamam ama burda olan olay ne?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec605ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.19453217, -0.7936975 ,  1.23878508],\n",
       "        [-1.52211801, -0.3633141 , -1.09612726],\n",
       "        [-0.27881969,  0.74103717,  1.03219534],\n",
       "        [ 0.63763537,  1.08308935, -0.18640772],\n",
       "        [-0.56533241, -0.01329518, -1.90528166],\n",
       "        [-0.08785873, -2.34340253, -0.50334106],\n",
       "        [ 0.84780722, -0.87618371,  0.19949783],\n",
       "        [-0.50037419, -0.02923432,  1.64864089],\n",
       "        [-0.56440474, -1.35151236,  0.30333497],\n",
       "        [-0.0813958 , -0.46163601, -0.8162573 ]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an RNN\n",
    "# number of hidden units M\n",
    "M = 5\n",
    "\n",
    "i = Input(shape = (T,D))\n",
    "x = SimpleRNN(M)(i) #we assumed default activation which is tanh\n",
    "x = Dense(K)(x) #let's assume we're doing regression so there is no activation function\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8fd04d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 217ms/step\n",
      "[[ 0.28285617 -0.09928322]]\n"
     ]
    }
   ],
   "source": [
    "#use model to make a pred\n",
    "Yhat = model.predict(X)\n",
    "print(Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43eb4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hocanınki 1x2 çıktı benim 10x2 niye? bende her timestamp için ayrıca mı hesapladı?\n",
    "#?SimpleRNN\n",
    "# e ama return sequences default'ta false da gözüküyo ne mana\n",
    "#çünkü gerizekalı dense layer'ı x yerine i'ye bağlamışsın şimdi düzeldi thx to chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c97c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 10, 3)]           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 5)                 45        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac53e08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.83532584, -0.6514124 , -0.02678806, -0.72216   , -0.79930604],\n",
       "        [-0.29870445,  0.13089561,  0.59346145,  0.21358949, -0.29906106],\n",
       "        [-0.06798428, -0.69046766, -0.79839563, -0.35065335,  0.251105  ]],\n",
       "       dtype=float32),\n",
       " array([[ 0.05338168,  0.83906746, -0.18806717, -0.48868334, -0.13760741],\n",
       "        [ 0.42733547, -0.09336749, -0.15131587, -0.2911593 ,  0.8372555 ],\n",
       "        [-0.5687558 , -0.20128128,  0.47295716, -0.6277616 ,  0.13501695],\n",
       "        [ 0.02498548, -0.4723776 , -0.6730736 , -0.45234   , -0.34437704],\n",
       "        [-0.7003053 ,  0.15360272, -0.5147986 ,  0.27878094,  0.3784738 ]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## his is a model with random weights and biases\n",
    "model.layers[1].get_weights()\n",
    "\n",
    "#it looks like three big arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21d263a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (5, 5) (5,)\n"
     ]
    }
   ],
   "source": [
    "#it's actually more helpful to print shapes of these arrays:\n",
    "# first output is input > hidden\n",
    "# second output is hidden > hidden\n",
    "# third output is bias term\n",
    "\n",
    "a,b,c = model.layers[1].get_weights()\n",
    "print(a.shape, b.shape, c.shape)\n",
    "\n",
    "#the first weight is 3x5, input to hidden layer\n",
    "# the second weight is 5x5, hidden layer to hiddenlayer (recurrent)\n",
    "# third weight vector by M which means the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57419c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mantıklı çünkü M'i biz 5 belirlemiştik\n",
    "\n",
    "Wx, Wh, bh = model.layers[1].get_weights()\n",
    "Wo, bo = model.layers[2].get_weights() #dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47e2016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02319835 0.19658582]\n"
     ]
    }
   ],
   "source": [
    "#last step is to do our manual RNN calculation\n",
    "#we'll initialize a hidden state to a vector of zeroes\n",
    "#this is also our way to confirm that the initial hidden state is indeed zero by comparing two outputs\n",
    "\n",
    "h_last = np.zeros(M)\n",
    "x = X[0] #bcz we only had 1 sample for X, so this will select all the sample. 1den fazla sample'ımız olsa napcaktk ki?\n",
    "Yhats = [] #where we store the outputs\n",
    "\n",
    "for t in range(T):\n",
    "    h = np.tanh(x[t].dot(Wx) + h_last.dot(Wh) + bh)\n",
    "    y = h.dot(Wo) + bo #we'll only care about this value in the last iteration\n",
    "    Yhats.append(y)\n",
    "    \n",
    "    h_last = h\n",
    "    \n",
    "print(Yhats[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1448a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.05560619, -0.25817303]), array([ 0.7174579 , -0.82431744]), array([0.01325308, 1.03616249]), array([-0.79721722,  0.64520941]), array([-0.26528943, -0.69266743]), array([0.4197635 , 0.93329063]), array([-0.2404735 ,  0.12029364]), array([ 0.41897202, -0.78028751]), array([ 1.0837912 , -0.25283553]), array([0.02319835, 0.19658582])]\n"
     ]
    }
   ],
   "source": [
    "print(Yhats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2776b8ca",
   "metadata": {},
   "source": [
    "#### see this is equal to what we calculated before in step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a0616",
   "metadata": {},
   "source": [
    "### Bonus exercise: Calculate this for more than 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "defdc889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.36626404, -0.28749524, -0.67429819],\n",
       "        [-0.01822258,  0.98873318,  0.25819092],\n",
       "        [ 0.39025371, -1.17494434, -0.12687456],\n",
       "        [ 0.36452043,  0.24914082,  0.51245152],\n",
       "        [-0.40053118,  0.37266551,  1.01692962],\n",
       "        [-0.24503446, -0.71286245, -0.65378666],\n",
       "        [ 0.96050579,  0.59688585,  1.79240562],\n",
       "        [-0.21362317,  1.1411716 , -1.35714152],\n",
       "        [-0.95025084,  0.19518304,  0.92154138],\n",
       "        [ 0.83387336,  0.93824977, -0.86927701]],\n",
       "\n",
       "       [[ 0.66820928, -0.58017879, -0.93077255],\n",
       "        [ 1.01782246,  0.42371897, -0.11747005],\n",
       "        [-0.26584489, -0.69592804,  2.32856302],\n",
       "        [ 1.21116868,  0.68692477, -0.90206902],\n",
       "        [-1.08766561,  0.12870312,  0.84659945],\n",
       "        [ 0.10460487, -0.83208768,  0.69560535],\n",
       "        [ 0.21582375, -0.14485585,  1.43077286],\n",
       "        [ 0.85893891, -0.27599019,  1.56865603],\n",
       "        [-0.68035431,  0.67757608,  1.46120034],\n",
       "        [ 1.40912492,  0.13270857,  0.52291865]],\n",
       "\n",
       "       [[ 1.00246561,  1.04618832, -0.08967802],\n",
       "        [ 0.04958105, -0.08289896,  0.33482114],\n",
       "        [-0.79614402, -0.69511258,  2.74291434],\n",
       "        [ 1.94720235, -0.0623699 , -0.09947862],\n",
       "        [ 0.72732295,  0.62238721, -0.71224478],\n",
       "        [-0.16348119, -1.92582451, -1.53593782],\n",
       "        [-0.50546134,  0.32302334, -1.26667208],\n",
       "        [ 1.12374354,  0.4784896 , -0.14940465],\n",
       "        [-0.20488076,  0.62340546,  1.34977356],\n",
       "        [ 0.46336938,  0.76416523, -1.35861373]],\n",
       "\n",
       "       [[ 0.41893322,  1.72136638,  1.58942579],\n",
       "        [-0.62748409,  1.22191312,  0.11655987],\n",
       "        [-1.21435822, -1.62004715,  1.28779685],\n",
       "        [-0.2634615 ,  1.38152404, -0.06629035],\n",
       "        [-1.59692897,  0.70750307, -0.31591785],\n",
       "        [-1.44385884,  0.11964257, -0.02290234],\n",
       "        [-2.83590043, -0.10361951,  1.71866945],\n",
       "        [-0.52915893, -0.16947925,  0.06691961],\n",
       "        [-1.67593423,  1.17481534,  1.61663822],\n",
       "        [ 1.33389456,  2.08869179, -0.17383322]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make some data:\n",
    "N = 4 #sample\n",
    "T = 10 #sequence length\n",
    "D = 3 # feature number (vector dimensionality)\n",
    "K = 2\n",
    "X = np.random.randn(N, T, D)\n",
    "X\n",
    "\n",
    "#now I have 4 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26235adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5\n",
    "\n",
    "i = Input(shape = (T,D))\n",
    "x = SimpleRNN(M)(i) \n",
    "x = Dense(K)(x) #let's assume we're doing regression so there is no activation function\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05d6a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f888a942430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "[[ 1.5781996  -1.4572457 ]\n",
      " [ 0.47916812 -0.23345041]\n",
      " [ 1.6686987  -1.5694398 ]\n",
      " [ 0.9615597  -0.7691152 ]]\n"
     ]
    }
   ],
   "source": [
    "#use model to make predictions\n",
    "Yhat = model.predict(X)\n",
    "print(Yhat)\n",
    "\n",
    "#we have 2 outputs for each of 4 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af6d6a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 10, 3)]           0         \n",
      "                                                                 \n",
      " simple_rnn_4 (SimpleRNN)    (None, 5)                 45        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38317fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.5862994 ,  0.5813628 ,  0.7974624 ,  0.07803494, -0.7945263 ],\n",
       "        [-0.8436101 , -0.09644419,  0.68786484, -0.43702722, -0.7325891 ],\n",
       "        [ 0.3859269 , -0.4878196 ,  0.7217867 , -0.63964015,  0.15325826]],\n",
       "       dtype=float32),\n",
       " array([[-0.21672702,  0.35616493,  0.34219792,  0.6425723 , -0.54422176],\n",
       "        [-0.2823942 ,  0.53048223, -0.29434738,  0.3309667 ,  0.66532904],\n",
       "        [-0.23114593, -0.09281105,  0.8623884 , -0.12879865,  0.42149094],\n",
       "        [ 0.8877089 ,  0.3662093 ,  0.22412716,  0.09347865,  0.13744962],\n",
       "        [-0.17840238,  0.6700836 ,  0.04809683, -0.6724838 , -0.25419044]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()\n",
    "\n",
    "#tamam her sample için ayrı weight hesapşamamış mantıklı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eaebf6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (5, 5) (5,)\n"
     ]
    }
   ],
   "source": [
    "a,b,c = model.layers[1].get_weights()\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca4debf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx, Wh, bh = model.layers[1].get_weights()\n",
    "Wo, bo = model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c721961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.57819971 -1.45724568]\n",
      " [ 0.48013484 -0.2339073 ]\n",
      " [ 1.66979636 -1.57084694]\n",
      " [ 0.96161535 -0.76916654]]\n"
     ]
    }
   ],
   "source": [
    "h_last = np.zeros(M)\n",
    "Yhats = np.zeros(((N,T,K))) #where we store the outputs #allahım vallahi kendim buldum şu şekil store etmeyi\n",
    "\n",
    "for i in range(N):\n",
    "    x = X[i]\n",
    "    for t in range(T):\n",
    "        h = np.tanh(x[t].dot(Wx) + h_last.dot(Wh) + bh)\n",
    "        y = h.dot(Wo) + bo \n",
    "        \n",
    "        Yhats[i,t] = y\n",
    "\n",
    "        h_last = h\n",
    "\n",
    "print(Yhats[:,-1])\n",
    "###AY OLDU OLDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c6e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
