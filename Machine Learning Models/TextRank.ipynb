{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856e9095",
   "metadata": {},
   "source": [
    "# TextRank\n",
    "\n",
    "### Recap Tf-Idf method\n",
    "- split doc into sentences\n",
    "- compute tfidf matrix (sentences x terms)\n",
    "- compute scores as average of nonzero values\n",
    "- take top scoring sentences\n",
    "\n",
    "#### TextRank is an alternative way for scoring sentences, instead of taking the average for each sentence, it use a more complex method, all the other steps remain the same\n",
    "\n",
    "## Google PageRank\n",
    "- TextRank is based on Google's PageRank\n",
    "- remember: internet is made of webpages each of which can potentially return as the search result\n",
    "- we wanna compute a score for each webpage\n",
    "- we can rank the search results according to their scores\n",
    "### How do we score each webpage?\n",
    "- through what is called random walk\n",
    "- start from an arbitrary page, select a link on that webpage, go to that page, and repeat this process after doing some math, repeat forever...\n",
    "- after the fancy map, magic happens: it turns out probability of landing on a webpage, after a certain amount of time remains constant\n",
    "- it doesn't matter where you start from!!!\n",
    "- no matter where you start if you keep going from webpage to webpage randomly, you'll eventually settle on some fixed distribution\n",
    "\n",
    "- Intuitviely a webpage with more incoming links will be more popular and will have a higher chance of being landed on\n",
    "- these probabilities are in fact page rank scores\n",
    "- the assumption is that more authoritative and legitimate pages will have more incoming links\n",
    "- what if people just make a bunch of website that link to their own website\n",
    "- As an exercise think why that won't work\n",
    "### Applying PageRank to TextRank (somewhat advanced)\n",
    "- treat every sentence as a webpage\n",
    "- what is the equivalent of incoming links?\n",
    "- the number of links from one sentence to another is the cosine similarity between their tfidf vectors!\n",
    "- Intuitviely if many sentences have high cosine similarity to one authoritative sentence, that authoritative sentence will end up in a higher score NOTE THIS IS NOT SYMMETRIC EVEN THOUGH THE COSINE SIMILARITY IS!\n",
    "- because the probability of walking from one sentence to another will depend on how many other sentences it is similar to\n",
    "- let's say authoritative sentence will have high cosine similarity to multiple other sentences, the probability of taking a random walk towards each will be much smaller, as the prob will spread among those sentences\n",
    "- as with PageRank, the probability of landing on a particular sentence will eventually settle on a fixed distributon\n",
    "- and these will be textrank scores\n",
    "\n",
    "## How does it really work? Mechanics of PageRank / TextRank\n",
    "- this is a really compressed version of what's going on, normalde buna ayrı bi ders grekir dedi\n",
    "### Random Walks and Markov Chains\n",
    "random walks is an example of Markov chains\n",
    "- markov chains made up of a set of states\n",
    "- we can go from any state at time t to another state in time t+1 according to some given probabilities\n",
    "- suppose we have M states in total (in this case M webpages)\n",
    "- the probability going from state i at time t, to state j in time t+1 : A(i,j) = p(st+1 = j| st=i)\n",
    "- this transition follow markov property (only depend on whre i was previously but not on steps before tht)\n",
    "- next step: how we represent the probability of being in a particular state?\n",
    "#### This is called State Distribution for each state\n",
    "- p(st) = [p(st =1), p(st=2)...p(st=M)]\n",
    "- so p(st) is a vector of probabilities values telling us the prob of being at a particular state at time t.\n",
    "- By convention, it's a 1xM row vector\n",
    "#### The Next State Distribution:\n",
    "- we can use the former with A matrix to compute the state distribution in the next step\n",
    "- p(st+1) = p(st) * A #convince yourself.\n",
    "- Mantıklı çünkü bir state'te olma prob'unu row vektör yaptık, e sonra A'nın kolonlarıyla çarpınca ikinci state'teki probability distribution çıkıyo\n",
    "#### The Limiting Distribution: state distribution after an infinitely long random walk\n",
    "- p(s0)*A*A*A... = lim p(s0) A^t :t sonsuza giderken\n",
    "- no guarantees that it will converge, or it exists\n",
    "- LUCKILY A TRICK TO GUARANTEE THIS DISTRIBUTION DOES EXIST AND FIND IT NOT IN INF NUMBER STEPS\n",
    "\n",
    "- t= sonsuzdan başlayalım p(s-sonsuz) = p(s-sonsuz)* A .ünkü sonsuz + 1 yine sonsuz.\n",
    "- this is a typical Eigenvalue problem: A*v = lambda*v \n",
    "- ps-sonsuz is an eigenvector, eigenvalue is 1.\n",
    "- bu denklemin sağ tarafı is not limiting distribution but a stationary distribution (a distribution that doesn't change after 1 transition by A)\n",
    "- question : how do we know that this stationary distribution is also a limiting distribution\n",
    "- how do we know it's unique?\n",
    "- how do we know the corresponding eigenvalue is 1, to begin with?\n",
    "#### Answer: Perron-Frobenius Theorem\n",
    "- if A is a Markov Matrix (stochastic matrix) and we can travel from any state to another state with positive probability, then all the assumptions above are true\n",
    "- you don't need to prove this theory, just to check if criteria are met\n",
    "1- A is a Markov Matrix (each rows sums to 1 and values are non-neg)\n",
    "\n",
    "2- All values ARE positive (non-neg not enough)\n",
    "- if 1 and 2 holds, A has an eigenvalue of 1, limiting dist is same as stationary dist and its unique\n",
    "\n",
    "- Normally in a webpage i, if the number of links are N, going to each of these links randomly from page i will be 1/N(i)\n",
    "- and of course the probability will be 0 for the pages which does not contained as link in page i\n",
    "- let's build a matrix G from these probabilities\n",
    "- since G contain zero values, it's not meeting Perron-Frobenius Theorem\n",
    "- What we'll do is we'll smooth matrix G by a uniform matrix U which contains 1/M everywhere to obtain a matrix A which meets the criterion\n",
    "- A = æ * U + (1-æ) * G \n",
    "- the sums will still be 1, and no zero values.\n",
    "\n",
    "#### Let's move on TextRank\n",
    "- we just computed tf-idf matrix\n",
    "- next: compute the cosine similarity pairwise between each sentence\n",
    "- This yields an MxM matrix that shows similarity between sentence i and sentence j\n",
    "- we cannot use it as it is since rows does not sum to 1.\n",
    "- divide each row by sum to enforce this constraint, now it's our G matrix\n",
    "- smooth G to get A\n",
    "\n",
    "## Exercise Prompt\n",
    "- only use Numpy but not any library for the advanced application\n",
    "- Hint: most code from previous notebook can be used\n",
    "- sentence tokenization, tfidf, ranking by scoe are same\n",
    "- conventional eigenvector representation is transpose of how it's presented in markov chains\n",
    "p = p*A\n",
    "- since we treat the state distribution as a row vector, this chage how we call eigenvalue function in np\n",
    "A^T * p^T = p^T\n",
    "\n",
    "### Solutions (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3866a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4f7773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yagmuraslan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yagmuraslan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cb405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'bbc_text_cls.csv' already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification\n",
    "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e78f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bbc_text_cls.csv')\n",
    "doc = df[df.labels == 'business']['text'].sample(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f07553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(x):\n",
    "  return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc267ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christmas sales worst since 1981\n",
      "\n",
      "UK retail sales fell in December,\n",
      "failing to meet expectations and making it by some counts the worst\n",
      "Christmas since 1981.\n",
      "\n",
      "Retail sales dropped by 1% on the month in\n",
      "December, after a 0.6% rise in November, the Office for National\n",
      "Statistics (ONS) said.  The ONS revised the annual 2004 rate of growth\n",
      "down from the 5.9% estimated in November to 3.2%. A number of\n",
      "retailers have already reported poor figures for December.  Clothing\n",
      "retailers and non-specialist stores were the worst hit with only\n",
      "internet retailers showing any significant growth, according to the\n",
      "ONS.\n",
      "\n",
      "The last time retailers endured a tougher Christmas was 23 years\n",
      "previously, when sales plunged 1.7%.\n",
      "\n",
      "The ONS echoed an earlier\n",
      "caution from Bank of England governor Mervyn King not to read too much\n",
      "into the poor December figures.  Some analysts put a positive gloss on\n",
      "the figures, pointing out that the non-seasonally-adjusted figures\n",
      "showed a performance comparable with 2003. The November-December jump\n",
      "last year was roughly comparable with recent averages, although some\n",
      "way below the serious booms seen in the 1990s.  And figures for retail\n",
      "volume outperformed measures of actual spending, an indication that\n",
      "consumers are looking for bargains, and retailers are cutting their\n",
      "prices.\n",
      "\n",
      "However, reports from some High Street retailers highlight\n",
      "the weakness of the sector.  Morrisons, Woolworths, House of Fraser,\n",
      "Marks & Spencer and Big Food all said that the festive period was\n",
      "disappointing.\n",
      "\n",
      "And a British Retail Consortium survey found that\n",
      "Christmas 2004 was the worst for 10 years.  Yet, other retailers -\n",
      "including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that\n",
      "festive sales were well up on last year.  Investec chief economist\n",
      "Philip Shaw said he did not expect the poor retail figures to have any\n",
      "immediate effect on interest rates.  \"The retail sales figures are\n",
      "very weak, but as Bank of England governor Mervyn King indicated last\n",
      "night, you don't really get an accurate impression of Christmas\n",
      "trading until about Easter,\" said Mr Shaw.  \"Our view is the Bank of\n",
      "England will keep its powder dry and wait to see the big picture.\"\n"
     ]
    }
   ],
   "source": [
    "print(wrap(doc.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb8da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UK retail sales fell in December, failing to meet expectations and making it by some counts the worst Christmas since 1981.\n",
      "\n",
      "Retail sales dropped by 1% on the month in December, after a 0.6% rise in November, the Office for National Statistics (ONS) said. The ONS revised the annual 2004 rate of growth down from the 5.9% estimated in November to 3.2%. A number of retailers have already reported poor figures for December. Clothing retailers and non-specialist stores were the worst hit with only internet retailers showing any significant growth, according to the ONS.\n",
      "\n",
      "The last time retailers endured a tougher Christmas was 23 years previously, when sales plunged 1.7%.\n",
      "\n",
      "The ONS echoed an earlier caution from Bank of England governor Mervyn King not to read too much into the poor December figures. Some analysts put a positive gloss on the figures, pointing out that the non-seasonally-adjusted figures showed a performance comparable with 2003. The November-December jump last year was roughly comparable with recent averages, although some way below the serious booms seen in the 1990s. And figures for retail volume outperformed measures of actual spending, an indication that consumers are looking for bargains, and retailers are cutting their prices.\n",
      "\n",
      "However, reports from some High Street retailers highlight the weakness of the sector. Morrisons, Woolworths, House of Fraser, Marks & Spencer and Big Food all said that the festive period was disappointing.\n",
      "\n",
      "And a British Retail Consortium survey found that Christmas 2004 was the worst for 10 years. Yet, other retailers - including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that festive sales were well up on last year. Investec chief economist Philip Shaw said he did not expect the poor retail figures to have any immediate effect on interest rates. \"The retail sales figures are very weak, but as Bank of England governor Mervyn King indicated last night, you don't really get an accurate impression of Christmas trading until about Easter,\" said Mr Shaw. \"Our view is the Bank of England will keep its powder dry and wait to see the big picture.\"\n"
     ]
    }
   ],
   "source": [
    "print(doc.iloc[0].split(\"\\n\", 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289951d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(doc.iloc[0].split(\"\\n\", 1)[1])\n",
    "\n",
    "featurizer = TfidfVectorizer(\n",
    "    stop_words=stopwords.words('english'),\n",
    "    norm='l1')\n",
    "\n",
    "X = featurizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22c7ee",
   "metadata": {},
   "source": [
    "#### buraya kadar Text Summarization notebook'un aynısıydı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17357d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute similarity matrix\n",
    "S = cosine_similarity(X) #bu fonksiyonu biliyoduk ama burda direkt matrikse uyguladık, a double for loop yapıyo\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16ccbca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116dac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize similarity matrix\n",
    "S /= S.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bd38004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0].sum() #prob'ar toplamı 1 ediyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c706a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform transition matrix\n",
    "U = np.ones_like(S) / len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6d27f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[0].sum() #bunun da prob'lar toplamı 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cf9b73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smoothed similarity matrix\n",
    "factor = 0.15\n",
    "S = (1 - factor) * S + factor * U\n",
    "S[0].sum() #haliyle bunun da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7c7c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the limiting / stationary distribution\n",
    "#he'll show both eigenvalue and eigenvector since both work\n",
    "eigenvals, eigenvecs = np.linalg.eig(S.T) #we need to transpose our matrix because numpy implements the eig function\n",
    "#to work on column vectors by default, therefore we're transposing it to work on a row vector\n",
    "#6.dk da açıkılıyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "762e107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.24245466, 0.72108199, 0.67644122, 0.34790129,\n",
       "       0.34417302, 0.3866884 , 0.40333562, 0.41608572, 0.44238593,\n",
       "       0.63909999, 0.62556792, 0.58922572, 0.57452382, 0.48511399,\n",
       "       0.51329157, 0.52975372])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvals #recall one of these eigenvalues should be one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3baad03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24206557, -0.27051337, -0.2213806 , -0.28613638, -0.25065894,\n",
       "       -0.2499217 , -0.279622  , -0.21515455, -0.2226665 , -0.22745415,\n",
       "       -0.2059112 , -0.20959727, -0.23526242, -0.24203809, -0.23663025,\n",
       "       -0.2940483 , -0.20865607])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvecs[:,0] #next step is to check the corrsponding eigenvector\n",
    "#probably not good to assume that the corresponding eigenvector to eigenvalue 1 will be found in position 0.\n",
    "#better to index explicitly\n",
    "# we need to index the first column bcz eigenvectors goes by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "691ef56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24206557, -0.27051337, -0.2213806 , -0.28613638, -0.25065894,\n",
       "       -0.2499217 , -0.279622  , -0.21515455, -0.2226665 , -0.22745415,\n",
       "       -0.2059112 , -0.20959727, -0.23526242, -0.24203809, -0.23663025,\n",
       "       -0.2940483 , -0.20865607])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvecs[:,0].dot(S) #sanity check: bakalım bu eigenvector yü matrix'le çarpınca gene aynı eigenvector mü gelcek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4ff7c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05907327, 0.06601563, 0.05402535, 0.06982824, 0.06117038,\n",
       "       0.06099047, 0.06823848, 0.05250595, 0.05433915, 0.05550753,\n",
       "       0.05025022, 0.05114976, 0.05741304, 0.05906657, 0.05774684,\n",
       "       0.07175905, 0.05092007])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size bu eigenvector yanlış gelmiş olabilir, değerler negatif, ama unutma eigenvectorler tek değil ve \n",
    "# herhangi bi sayıyla çarpılabilir (öyle mi?) \n",
    "#new eigenvector will still be parallel with the original\n",
    "#bu şekilde 0-1 arası ve toplamı 1 eden pozitif olmasını sağlıycaz\n",
    "eigenvecs[:,0] / eigenvecs[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8c29f",
   "metadata": {},
   "source": [
    "#### Now here is a simpler way to find the same limiting distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f337c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "limiting_dist = np.ones(len(S)) / len(S) #to initiliza to be uniform dist\n",
    "threshold = 1e-8\n",
    "delta = float('inf')\n",
    "iters = 0\n",
    "while delta > threshold:\n",
    "  iters += 1\n",
    "\n",
    "  # Markov transition\n",
    "  p = limiting_dist.dot(S)\n",
    "\n",
    "  # compute change in limiting distribution\n",
    "  delta = np.abs(p - limiting_dist).sum()\n",
    "\n",
    "  # update limiting distribution\n",
    "  limiting_dist = p\n",
    "\n",
    "print(iters)\n",
    "\n",
    "#it only took 41 steps? \n",
    "#to convergee??  anlamadım bu kodu hi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3344bb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05907327, 0.06601563, 0.05402534, 0.06982824, 0.06117038,\n",
       "       0.06099047, 0.06823848, 0.05250595, 0.05433915, 0.05550753,\n",
       "       0.05025022, 0.05114977, 0.05741304, 0.05906657, 0.05774685,\n",
       "       0.07175905, 0.05092008])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limiting_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71f04d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9964739014777244e-08"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(eigenvecs[:,0] / eigenvecs[:,0].sum() - limiting_dist).sum() #logical this is where we set the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84653496",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = limiting_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e3460d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ca28445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary:\n",
      "0.07: \"The retail sales figures are very weak, but as Bank of England\n",
      "governor Mervyn King indicated last night, you don't really get an\n",
      "accurate impression of Christmas trading until about Easter,\" said Mr\n",
      "Shaw.\n",
      "0.07: A number of retailers have already reported poor figures for\n",
      "December.\n",
      "0.07: The ONS echoed an earlier caution from Bank of England governor\n",
      "Mervyn King not to read too much into the poor December figures.\n",
      "0.07: Retail sales dropped by 1% on the month in December, after a\n",
      "0.6% rise in November, the Office for National Statistics (ONS) said.\n",
      "0.06: Clothing retailers and non-specialist stores were the worst hit\n",
      "with only internet retailers showing any significant growth, according\n",
      "to the ONS.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated summary:\")\n",
    "for i in sort_idx[:5]:\n",
    "  print(wrap(\"%.2f: %s\" % (scores[i], sents[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b16443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, factor = 0.15):\n",
    "  # extract sentences\n",
    "  sents = nltk.sent_tokenize(text)\n",
    "\n",
    "  # perform tf-idf\n",
    "  featurizer = TfidfVectorizer(\n",
    "      stop_words=stopwords.words('english'),\n",
    "      norm='l1')\n",
    "  X = featurizer.fit_transform(sents)\n",
    "\n",
    "  # compute similarity matrix\n",
    "  S = cosine_similarity(X)\n",
    "\n",
    "  # normalize similarity matrix\n",
    "  S /= S.sum(axis=1, keepdims=True)\n",
    "\n",
    "  # uniform transition matrix\n",
    "  U = np.ones_like(S) / len(S)\n",
    "\n",
    "  # smoothed similarity matrix\n",
    "  S = (1 - factor) * S + factor * U\n",
    "\n",
    "  # find the limiting / stationary distribution\n",
    "  eigenvals, eigenvecs = np.linalg.eig(S.T)\n",
    "\n",
    "  # compute scores\n",
    "  scores = eigenvecs[:,0] / eigenvecs[:,0].sum()\n",
    "  \n",
    "  # sort the scores\n",
    "  sort_idx = np.argsort(-scores)\n",
    "\n",
    "  # print summary\n",
    "  for i in sort_idx[:5]:\n",
    "    print(wrap(\"%.2f: %s\" % (scores[i], sents[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dedb3d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11: Goodrem, Green Day and the Black Eyed Peas took home two awards\n",
      "each.\n",
      "0.10: As well as best female, Goodrem also took home the Pepsi Viewers\n",
      "Choice Award, whilst Green Day bagged the prize for best rock video\n",
      "for American Idiot.\n",
      "0.10: Other winners included Green Day, voted best group, and the\n",
      "Black Eyed Peas.\n",
      "0.10: The Black Eyed Peas won awards for best R 'n' B video and\n",
      "sexiest video, both for Hey Mama.\n",
      "0.10: Local singer and songwriter Missy Higgins took the title of\n",
      "breakthrough artist of the year, with Australian Idol winner Guy\n",
      "Sebastian taking the honours for best pop video.\n"
     ]
    }
   ],
   "source": [
    "doc = df[df.labels == 'entertainment']['text'].sample(random_state=123)\n",
    "summarize(doc.iloc[0].split(\"\\n\", 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "937de325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Goodrem wins top female MTV prize'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.iloc[0].split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6c961",
   "metadata": {},
   "source": [
    "### Solution : Beginner - with a library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f68425d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from sumy) (3.7)\n",
      "Requirement already satisfied: requests>=2.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from sumy) (2.28.1)\n",
      "Collecting breadability>=0.1.20\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docopt<0.7,>=0.6.1\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pycountry>=18.2.23\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: chardet in /opt/anaconda3/lib/python3.9/site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in /opt/anaconda3/lib/python3.9/site-packages (from breadability>=0.1.20->sumy) (4.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (4.64.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from pycountry>=18.2.23->sumy) (63.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (3.3)\n",
      "Building wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21695 sha256=8c91a7dd304be24d347e6f65f6e5740c731c3ae113c9339a4ac864f99f06e340\n",
      "  Stored in directory: /Users/yagmuraslan/Library/Caches/pip/wheels/ba/9f/70/7795228568b81b57a8932755938da9fb1f291b0576752604aa\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=54d4e31d3cb9ce1f9ca5184da377258fe74e35d70bef02319bfc494e3839ecdb\n",
      "  Stored in directory: /Users/yagmuraslan/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=07623b2efb2f8f234942ac6d532f9b5f7280adaf94ba6a75ca6733d79ace882d\n",
      "  Stored in directory: /Users/yagmuraslan/Library/Caches/pip/wheels/47/15/92/e6dc85fcb0686c82e1edbcfdf80cfe4808c058813fed0baa8f\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f152cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4bf277b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Sentence: The 21-year-old singer won the award for best female artist, with Australian Idol runner-up Shannon Noll taking the title of best male at the ceremony.>,\n",
       " <Sentence: As well as best female, Goodrem also took home the Pepsi Viewers Choice Award, whilst Green Day bagged the prize for best rock video for American Idiot.>,\n",
       " <Sentence: The Black Eyed Peas won awards for best R 'n' B video and sexiest video, both for Hey Mama.>,\n",
       " <Sentence: Local singer and songwriter Missy Higgins took the title of breakthrough artist of the year, with Australian Idol winner Guy Sebastian taking the honours for best pop video.>,\n",
       " <Sentence: The ceremony was held at the Luna Park fairground in Sydney Harbour and was hosted by the Osbourne family.>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = TextRankSummarizer() #this doesn't take text directly as nlp can work in many languages\n",
    "\n",
    "#the next function will take a text parser object and a tokenizer object:\n",
    "parser = PlaintextParser.from_string(\n",
    "    doc.iloc[0].split(\"\\n\", 1)[1],\n",
    "    Tokenizer(\"english\"))\n",
    "\n",
    "summary = summarizer(parser.document, sentences_count=5) #call summirizer parse the doc and tell how many sentences return\n",
    "summary\n",
    "#this lib also has different methods to decide which sentence to keep top percent etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba724576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 21-year-old singer won the award for best female artist, with\n",
      "Australian Idol runner-up Shannon Noll taking the title of best male\n",
      "at the ceremony.\n",
      "As well as best female, Goodrem also took home the Pepsi Viewers\n",
      "Choice Award, whilst Green Day bagged the prize for best rock video\n",
      "for American Idiot.\n",
      "The Black Eyed Peas won awards for best R 'n' B video and sexiest\n",
      "video, both for Hey Mama.\n",
      "Local singer and songwriter Missy Higgins took the title of\n",
      "breakthrough artist of the year, with Australian Idol winner Guy\n",
      "Sebastian taking the honours for best pop video.\n",
      "The ceremony was held at the Luna Park fairground in Sydney Harbour\n",
      "and was hosted by the Osbourne family.\n"
     ]
    }
   ],
   "source": [
    "for s in summary:\n",
    "    print(wrap(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b187ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodrem, known in both Britain and Australia for her role as Nina\n",
      "Tucker in TV soap Neighbours, also performed a duet with boyfriend\n",
      "Brian McFadden.\n",
      "Other winners included Green Day, voted best group, and the Black Eyed\n",
      "Peas.\n",
      "Goodrem, Green Day and the Black Eyed Peas took home two awards each.\n",
      "As well as best female, Goodrem also took home the Pepsi Viewers\n",
      "Choice Award, whilst Green Day bagged the prize for best rock video\n",
      "for American Idiot.\n",
      "Artists including Carmen Electra, Missy Higgins, Kelly Osbourne, Green\n",
      "Day, Ja Rule and Natalie Imbruglia gave live performances at the\n",
      "event.\n"
     ]
    }
   ],
   "source": [
    "summarizer = LsaSummarizer() ### latent semantic analysis summarizer, also included in sumie package\n",
    "#all the summarizer in this library has the same API so he won't explain syntax again\n",
    "summary = summarizer(parser.document, sentences_count=5)\n",
    "for s in summary:\n",
    "    print(wrap(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "495d58f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/9bnp6m5d7db0pky3235l4wgh0000gn/T/ipykernel_42422/1433638454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     Otherwise joined strings will bwe returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "#Imstead of dealing with parsers and summarizers, gensim has a method that you can directly pass the test\n",
    "#It also uses TextRank\n",
    "#it happens to use a variation of similarity fnction\n",
    "\n",
    "# https://radimrehurek.com/gensim_3.8.3/summarization/summariser.html (documentation)\n",
    "# https://arxiv.org/abs/1602.03606\n",
    "\n",
    "# Parameters\n",
    "# text (str) – Given text.\n",
    "# ratio (float, optional) – Number between 0 and 1 that determines the\n",
    "#     proportion of the number of sentences of the original text to be\n",
    "#     chosen for the summary.\n",
    "# word_count (int or None, optional) – Determines how many words will the\n",
    "#     output contain. If both parameters are provided, the ratio will be\n",
    "#     ignored.\n",
    "# split (bool, optional) – If True, list of sentences will be returned.\n",
    "#     Otherwise joined strings will bwe returned.\n",
    "import gensim\n",
    "from gensim.summarization.summarizer import summarize\n",
    "summary = summarize(doc.iloc[0].split(\"\\n\", 1)[1])\n",
    "print(wrap(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f93d1",
   "metadata": {},
   "source": [
    "they’ve said that the gensim.summarization module has been removed in versions Gensim 4.x because it was an unmaintained third-party module.\n",
    "\n",
    "To continue using gensim.summarization, you will have to downgrade the version of Gensim in requirements.txt. Try replacing it with gensim==3.8.3 or older."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8a080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
